---
title: "stat-471-final-proj"
author: "Ethan Kallett"
date: "12/19/2021"
output:
  bookdown::pdf_document2:
    number_sections: yes
    toc: yes
    toc_depth: '3'
urlcolor: blue
---

# Link to GitHub Repo

https://github.com/ethanrimes/stat-471-final-project

# Executive Summary

## Problem

In 1933, after Josef Stalin replaced the then-deceased Vladimir Lenin as premier of the Soviet Union, he looked around at the much wealthier nations to his West and noticed one key commonality – Great Britain, France and Germany all had well-developed industrial sectors. Stalin made the reallocation of resources from agriculture to industry perhaps the defining feature of his three decades of rule, even when it came at the cost of the lives of 35+ million of his own citizens.

This study sought to measure the impact of a “Big Push” – a massive reallocation of capital and labor from less productive (lower value-added) segments of the economy to higher value-added 

## Data

The majority of the data came from the World Bank Development Indicators. They are one of the world’s most comprehensive datasets on economic development by country, with nearly 1500 indicators across 90 topics gathered annually at the country level since 1960. However, since there was a sparsity of data on some indicators, I chose indicators which contained a minimum threshold of data and imputed values for the remaining NA values.

Additionally, since I believed that data relating to armed conflict would be very relevant, I obtained this data from the Uppsala Conflict Data Program.

## Analysis

The first part of my project consists of an exploratory data analysis that attempts to understand the relationship between the allocation of resources across the major sectors of an economy and the level of economic development. I do this through the use of many summary statistics and graphs that explore the distribution of the labor across agriculture, industry and GDP per capita, as well as create a Recurrent Neural Network (RNN) to model this relationship.

The main focus of this assignment attempts to look at the relationship between the allocation of these resources and economic growth. I first pull 27 different covariates from the World Bank Development Indicators and a 28th focused on the level of armed conflict in different countries. First, I conducted a panel regression (through the plm package) of real GDP per capita growth (over various time periods) on the covariates identified and the different numbers of lags. Following this, I create a Recurrent Neural Network that predicts GDP per capita growth using the 28 features and a time length of 5 years for the samples. 

## Conclusions

First, it is quite evident that a linear model is much more better suited for modeling this specific problem than a neural network. 

Second, in terms of the interpretation of the models, the most important feature in determining economic growth would be capital accumulation per worker. This would suggest there was something correct about Joseph Stalin’s cruel logic, and that, in order to grow, an economy may have to reallocate resources to build up capital in a “big push”. Beyond this feature, other features such as natural resource rents, Gini index (inequality) and imports of goods and services are also very influential.

# Introduction

## Background Information

The original motivation for this topic was my senior thesis in economics. Attempts to assess how the “China shock” heterogeneously impacts Brazil through trade. The enormity of Chinese demand for commodities, in particular soy, petroleum and iron, coupled with the prohibitively competitive efficiency of Chinese manufacturing, has led to an enormous shift in the focus of the Brazilian economy from industry to agriculture and mining over the past 20 years. This is not problematic in itself, but one of the main concerns of this “deindustrialization” of the country is that it is a huge barrier to growth. I wanted to use some of the models and methods I had learned in class to see if this deindustrialization will have economic consequences for a country in the long run. 

Everything I learned about Stalinism in ECON 271: Foundations of a Market Economy just further stimulated my interest. In that class, we researched and debated quite a bit about whether or not Stalinism and the 30+ million deaths associated with it was necessary to create the relatively modern industrial superpower capable of defeating Nazi Germany in World War Two. 

In addition, a huge motivation for the initial exploratory data analysis of this assignment was based on the very broad generalization of agricultural economies (e.g. Chad, Kenya, Ghana, Afghanistan) as poor economies, industrial economies (e.g. China, Thailand, Malaysia, Mexico) as middle income economies and service economies (e.g. the US, Japan, the UK, Israel) as wealthy economies. This logic follows from the fact that the amount of value added per labor hour increases in that order, so therefore the wage rate of economies should increase in that order.

## Analysis Goals
To put it succinctly, my principal analysis goals were to understand the relationship between the allocation of resources in an economy and its economic growth in the medium term. As a secondary area of interest, I wanted to understand how precisely the allocation of resources across various sectors of an economy (agriculture, industry and services) was able to determine it’s level of economic development. 

For the initial data exploration, in which I wanted to understand the strength of the relationship between the allocation of resources across different economic sectors and the country’s economic development, I used the allocation of labor across the three broad sectors of an economy (agriculture, industry and services) as the main features to predict the response. The data on this allocation of labor was surprisingly abundant. The response that I aimed to predict was GDP per capita, measured in current 2021 US dollars at purchasing power parity (PPP).

In the main focus of the assignment, I chose a selection of 28 different features from the World Bank Development Index that intended to either assess the allocation of resources within an economy or control for omitted variable bias. The outcome that I tried to predict was the percentage difference in an economy over the past several years. For the size/duration of this time frame, I tried a period of 1, 3, 5 and 10 years. 

Success is evaluated in various aspects. The first metric is test MSE, i.e. how well is the model able to predict outcomes on data that it has not been trained on. This is a good approximation of how well the model was able to fit the underlying data generating process. The numerical, objective nature of this metric also makes it great to compare different models. The second criterion is how well the models can differentiate between the different importances of different features. If the end goal is to understand what allocation of resources within an economy will help it grow the most in the medium term, it’s crucial to understand what levers are available to pull, which direction to pull them, and which levers are the most powerful.

## Significance

As countries across the world, particularly emerging market economies, strive to achieve sustainable growth to improve the quality of life of their citizens over the long run, studies like these are crucial. If a model can have a very low test MSE on the empirical data (meaning that it represents very well the underlying data generating process) and it is able to distinguish well between the features of which it is a part (indicating what are the important levers a “social planner” should pull), such a model would have enormous practical applications for governments across the world. Economic development leads to better economic lives for the majority of citizens of a country and aggregate gains in welfare can be huge.


```{r setup, cache = TRUE, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = TRUE, lazy.cache = FALSE)
library(tidyverse)
library(cowplot)
library(rnn)
library(caret)
library(keras)
library(plm)
library(kableExtra)
source("/Users/ethan/Documents/R/stat-471-fall-2021/stat-471-fall-2021/functions/deep_learning_helpers.R")
```

```{r imports, cache = TRUE, include=FALSE}
# a-z = 26 + aa-az = 26 + ba-bn = 14, sum = 66
# country name, country code, indicator name, indicator code, 1960-2020, empty final col
wdi_raw <- 
  read_csv("/Users/ethan/Documents/R/stat-471-final-project/WDI_csv/WDIData.csv",
           col_names = TRUE)
# for country code, (country) table name, region, income group, latest industrial data (year), latest trade data (year)
wdi_country <- 
  read_csv("/Users/ethan/Documents/R/stat-471-final-project/WDI_csv/WDICountry.csv",
           col_names = TRUE)
# for series code, topic, indicator name, short definition, long definition, periodicity, aggregation method
wdi_series <- 
  read_csv("/Users/ethan/Documents/R/stat-471-final-project/WDI_csv/WDISeries.csv",
           col_names = TRUE)
# conflict 
conflict_df <- 
  read_csv("/Users/ethan/Documents/R/stat-471-final-project/conflict.csv",
           col_names = TRUE)
```
```{r plots-p1-p4, cache = TRUE, include=FALSE}
# this part is almost an EDA

# let's create a table showing average shares by income group over time
holy_trinity <- wdi_raw %>% 
  filter(`Indicator Code` %in% c("SL.SRV.EMPL.ZS", 
                                 "SL.IND.EMPL.ZS", 
                                 "SL.AGR.EMPL.ZS"))

summary_trinity_time_income_group <- wdi_country %>% 
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`) %>%
  inner_join(holy_trinity, by = c("Short Name" = "Country Name")) %>%
  group_by(`Short Name`, `Indicator Code`, `Income Group`) %>%
  summarise(`90s` = mean(`1991`,`1992`,`1993`,`1994`,`1995`,`1996`,
                         `1997`,`1998`,`1999`, na.rm=TRUE),
            `00s` = mean(`2000`,`2001`,`2002`,`2003`,`2004`,`2005`,`2006`,`2007`,
                         `2008`,`2009`, na.rm=TRUE),
            `10s` = mean(`2010`,`2011`,`2012`,`2013`,`2014`,`2015`,`2016`,
                         `2017`,`2018`,`2019`, na.rm=TRUE))

summary_trinity_time_income_group_90s <- 
  summary_trinity_time_income_group %>%
  select(`Indicator Code`, `Income Group`, `90s`) %>%
  pivot_wider(names_from = `Indicator Code`, values_from = `90s`) %>%
  transmute(`Income Group` = as.factor(`Income Group`), 
            `Agriculture` = SL.AGR.EMPL.ZS, # change variable names
            `Industry` = SL.IND.EMPL.ZS,
            `Services` = SL.SRV.EMPL.ZS) %>%
  pivot_longer(cols = c(`Agriculture`,
                        `Industry`,
                        `Services`),
               values_to = "value",
               names_to = "Sector") %>%
  group_by(`Income Group`, Sector) %>% 
  summarise(value = mean(value, na.rm =TRUE)) %>% # calculate average share per sector
  ungroup() %>%
  mutate(income_group_order = case_when( # order the income groups
    `Income Group` == "Low income" ~ 1,
    `Income Group` == "Lower middle income" ~ 2,
    `Income Group` == "Upper middle income" ~ 3,
    `Income Group` == "High income" ~ 4
  ))
p1 <- summary_trinity_time_income_group_90s %>%
  ggplot(aes(x = Sector, y = value, fill = 
               fct_reorder(.f = summary_trinity_time_income_group_90s$`Income Group`, .x = summary_trinity_time_income_group_90s$income_group_order))) +
  geom_bar(position="dodge", stat="identity") +
  labs(x = "Sector", y = "Share of Employment",
       title = "1991-1999 Average Employment by Sector by Income Group") +
  guides(fill=guide_legend(title="Income Group")) +
  theme(text = element_text(size=9),
        legend.key.size = unit(0.5, 'cm'),
        plot.title = element_text(hjust=0.5)) 
  
summary_trinity_time_income_group_00s <- 
  summary_trinity_time_income_group %>%
  select(`Indicator Code`, `Income Group`, `00s`) %>%
  pivot_wider(names_from = `Indicator Code`, values_from = `00s`) %>%
  transmute(`Income Group` = as.factor(`Income Group`), 
            `Agriculture` = SL.AGR.EMPL.ZS, # change variable names
            `Industry` = SL.IND.EMPL.ZS,
            `Services` = SL.SRV.EMPL.ZS) %>%
  pivot_longer(cols = c(`Agriculture`,
                        `Industry`,
                        `Services`),
               values_to = "value",
               names_to = "Sector") %>%
  group_by(`Income Group`, Sector) %>% 
  summarise(value = mean(value, na.rm =TRUE)) %>% # calculate average share per sector
  ungroup() %>%
  mutate(income_group_order = case_when( # order the income groups
    `Income Group` == "Low income" ~ 1,
    `Income Group` == "Lower middle income" ~ 2,
    `Income Group` == "Upper middle income" ~ 3,
    `Income Group` == "High income" ~ 4
  ))
p2 <- summary_trinity_time_income_group_00s %>%
  ggplot(aes(x = Sector, y = value, fill = 
               fct_reorder(.f = summary_trinity_time_income_group_00s$`Income Group`, .x = summary_trinity_time_income_group_00s$income_group_order))) +
  geom_bar(position="dodge", stat="identity") +
  labs(x = "Sector", y = "Share of Employment",
       title = "2000-2009 Average Employment by Sector by Income Group") +
  guides(fill=guide_legend(title="Income Group")) +
  theme(text = element_text(size=9),
        legend.key.size = unit(0.5, 'cm'),
        plot.title = element_text(hjust=0.5)) 

summary_trinity_time_income_group_10s <- 
  summary_trinity_time_income_group %>%
  select(`Indicator Code`, `Income Group`, `10s`) %>%
  pivot_wider(names_from = `Indicator Code`, values_from = `10s`) %>%
  transmute(`Income Group` = as.factor(`Income Group`), 
            `Agriculture` = SL.AGR.EMPL.ZS, # change variable names
            `Industry` = SL.IND.EMPL.ZS,
            `Services` = SL.SRV.EMPL.ZS) %>%
  pivot_longer(cols = c(`Agriculture`,
                        `Industry`,
                        `Services`),
               values_to = "value",
               names_to = "Sector") %>%
  group_by(`Income Group`, Sector) %>% 
  summarise(value = mean(value, na.rm =TRUE)) %>% # calculate average share per sector
  ungroup() %>%
  mutate(income_group_order = case_when( # order the income groups
    `Income Group` == "Low income" ~ 1,
    `Income Group` == "Lower middle income" ~ 2,
    `Income Group` == "Upper middle income" ~ 3,
    `Income Group` == "High income" ~ 4
  ))
p3 <- summary_trinity_time_income_group_10s %>%
  ggplot(aes(x = Sector, y = value, fill = 
               fct_reorder(.f = summary_trinity_time_income_group_10s$`Income Group`, .x = summary_trinity_time_income_group_10s$income_group_order))) +
  geom_bar(position="dodge", stat="identity") +
  labs(x = "Sector", y = "Share of Employment",
       title = "2010-2019 Average Employment by Sector by Income Group") +
  guides(fill=guide_legend(title="Income Group")) +
  theme(text = element_text(size=9),
        legend.key.size = unit(0.5, 'cm'),
        plot.title = element_text(hjust=0.5)) 
```
```{r share-change-over-time, cache = TRUE, include=FALSE}
# lets see how the shares of each sector evolve over time
shares_change_over_time <- wdi_country %>% 
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`) %>%
  inner_join(holy_trinity, by = c("Short Name" = "Country Name")) %>% 
  pivot_longer(cols=`1991`:`2019`, names_to = "year", values_to = "share") %>%
  select(`Short Name`, `Income Group`, `Indicator Code`, year, share) %>%
  drop_na() %>%
  group_by(`Indicator Code`, year, `Income Group`) %>%
  summarise(share = mean(share)) %>%
  transmute(year = year, share = share,`Income Group`=`Income Group`,
            indicator = as.factor(case_when(
              `Indicator Code` == "SL.AGR.EMPL.ZS" ~ "Agriculture",
              `Indicator Code` == "SL.IND.EMPL.ZS" ~ "Industry",
              `Indicator Code` == "SL.SRV.EMPL.ZS" ~ "Services"
            )))
shares_change_over_time %>%
  ggplot(aes(x = year, y = share, colour=indicator)) +
  geom_point() +
  facet_wrap(~`Income Group`) +
  scale_x_discrete(breaks = seq(1991, 2019, by = 5))
  
```
```{r shares-change-tibble, cache = TRUE, include=FALSE}
high_inc_agr_1991 =  shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 1991,
         indicator == "Agriculture") %>% pull(share)
high_inc_agr_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 2019,
         indicator == "Agriculture") %>% pull(share)
high_inc_ind_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 1991,
         indicator == "Industry") %>% pull(share)
high_inc_ind_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 2019,
         indicator == "Industry") %>% pull(share)
high_inc_srv_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 1991,
         indicator == "Services") %>% pull(share)
high_inc_srv_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 2019,
         indicator == "Services") %>% pull(share)

highmid_inc_agr_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 1991,
         indicator == "Agriculture") %>% pull(share)
highmid_inc_agr_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 2019,
         indicator == "Agriculture") %>% pull(share)
highmid_inc_ind_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 1991,
         indicator == "Industry") %>% pull(share)
highmid_inc_ind_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 2019,
         indicator == "Industry") %>% pull(share)
highmid_inc_srv_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 1991,
         indicator == "Services") %>% pull(share)
highmid_inc_srv_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 2019,
         indicator == "Services") %>% pull(share)

lowmid_inc_agr_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 1991,
         indicator == "Agriculture") %>% pull(share)
lowmid_inc_agr_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 2019,
         indicator == "Agriculture") %>% pull(share)
lowmid_inc_ind_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 1991,
         indicator == "Industry") %>% pull(share)
lowmid_inc_ind_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 2019,
         indicator == "Industry") %>% pull(share)
lowmid_inc_srv_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 1991,
         indicator == "Services") %>% pull(share)
lowmid_inc_srv_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 2019,
         indicator == "Services") %>% pull(share)

low_inc_agr_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 1991,
         indicator == "Agriculture") %>% pull(share)
low_inc_agr_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 2019,
         indicator == "Agriculture") %>% pull(share)
low_inc_ind_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 1991,
         indicator == "Industry") %>% pull(share)
low_inc_ind_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 2019,
         indicator == "Industry") %>% pull(share)
low_inc_srv_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 1991,
         indicator == "Services") %>% pull(share)
low_inc_srv_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 2019,
         indicator == "Services") %>% pull(share)

# percent change over time across sectors and income groups, precipitous decline in ag, increase in services
pct_change_groups_time <- tibble(
  income_group = c("High income", "Upper middle income", "Lower middle income",
                   "Low income"),
  change_agriculture = c((high_inc_agr_2019 - high_inc_agr_1991)/high_inc_agr_1991,
                         (highmid_inc_agr_2019 - highmid_inc_agr_1991)/highmid_inc_agr_1991,
                         (lowmid_inc_agr_2019 - lowmid_inc_agr_1991)/lowmid_inc_agr_1991,
                         (low_inc_agr_2019 - low_inc_agr_1991)/low_inc_agr_1991),
  change_industry = c((high_inc_ind_2019 - high_inc_ind_1991)/high_inc_ind_1991,
                         (highmid_inc_ind_2019 - highmid_inc_ind_1991)/highmid_inc_ind_1991,
                         (lowmid_inc_ind_2019 - lowmid_inc_ind_1991)/lowmid_inc_ind_1991,
                         (low_inc_ind_2019 - low_inc_ind_1991)/low_inc_ind_1991),
  change_services = c((high_inc_srv_2019 - high_inc_srv_1991)/high_inc_srv_1991,
                         (highmid_inc_srv_2019 - highmid_inc_srv_1991)/highmid_inc_srv_1991,
                         (lowmid_inc_srv_2019 - lowmid_inc_srv_1991)/lowmid_inc_srv_1991,
                         (low_inc_srv_2019 - low_inc_srv_1991)/low_inc_srv_1991)
)
```
```{r calculate-boxplot, cache = TRUE, include=FALSE}
# variance in each group and each time period
summary_trinity_time_income_group <- summary_trinity_time_income_group %>%
  mutate(income_group_order = case_when( # order the income groups
    `Income Group` == "Low income" ~ 1,
    `Income Group` == "Lower middle income" ~ 2,
    `Income Group` == "Upper middle income" ~ 3,
    `Income Group` == "High income" ~ 4
  ),
  Indicator = as.factor(case_when(
              `Indicator Code` == "SL.AGR.EMPL.ZS" ~ "Agriculture",
              `Indicator Code` == "SL.IND.EMPL.ZS" ~ "Industry",
              `Indicator Code` == "SL.SRV.EMPL.ZS" ~ "Services"
            )))

ninteties_boxplot <- summary_trinity_time_income_group %>%
  ggplot(aes(y = `90s`, x = fct_reorder(.f = summary_trinity_time_income_group$`Income Group`, .x = summary_trinity_time_income_group$income_group_order), colour=Indicator)) +
  geom_boxplot() +
  labs(x = "Income Group", y = "Share of Employment",
       title = "90s Distribution of Emp. by Sector by Income Group") +
  guides(fill=guide_legend(title="Sector")) +
  theme_bw()
  
millenium_boxplot <- summary_trinity_time_income_group %>%
  ggplot(aes(y = `00s`, x = fct_reorder(.f = summary_trinity_time_income_group$`Income Group`, .x = summary_trinity_time_income_group$income_group_order), colour=Indicator)) +
  geom_boxplot() +
  labs(x = "Income Group", y = "Share of Employment",
       title = "00s Distribution of Emp. by Sector by Income Group") +
  guides(fill=guide_legend(title="Sector")) +
  theme_bw()
  
tens_boxplot <- summary_trinity_time_income_group %>%
  ggplot(aes(y = `10s`, x = fct_reorder(.f = summary_trinity_time_income_group$`Income Group`, .x = summary_trinity_time_income_group$income_group_order), colour=Indicator)) +
  geom_boxplot() +
  labs(x = "Income Group", y = "Share of Employment",
       title = "10s Distribution of Emp. by Sector by Income Group") +
  guides(fill=guide_legend(title="Sector")) +
  theme(text = element_text(size=40),
        legend.key.size = unit(5, 'cm'),
        plot.title = element_text(hjust=0.5)) 
```

```{r calc-observations, cache = TRUE, include=FALSE}
# calculate number of observations to work with 
trinity_filtered <- wdi_country %>% 
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`) %>%
  inner_join(holy_trinity, by = c("Short Name" = "Country Name")) %>%
  select(-`Indicator Name`, -`Country Code`, -`2020`, -`...66`, -(`1960`:`1990`)) %>%
  drop_na()

trinity_filtered
# 516 rows, which means 172 countries with data from 1991-2019 (29 years of data)
```

```{r, cache = TRUE, include=FALSE}
# get data of employment shares and GDP
gdp_pcap_ppp_countries <- wdi_raw %>% 
  filter(`Indicator Code` %in% c("NY.GDP.PCAP.PP.CD")) %>%
  inner_join(
    (wdi_country %>% 
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`)),
    by = c("Country Name" = "Short Name")
  ) %>%
  select(`1991`:`2019`, `Country Name`, `Income Group`, `Indicator Code`) %>%
  rename(`Short Name` = `Country Name`)

within_model_data <- rbind(trinity_filtered, gdp_pcap_ppp_countries) %>%
  arrange(`Short Name`)

# remove the countries without economy share data
within_model_data <- 
  within_model_data %>% group_by(`Short Name`) %>% 
  summarise(count = n()) %>%
  ungroup() %>% 
  filter(count > 3) %>%
  select(-count) %>%
  inner_join(within_model_data,
             by = c("Short Name" = "Short Name"))

# value imputation
# replace all NA GDP values with the average GDP for the average in that income group for that year
average_GDP_stats <- within_model_data %>% group_by(`Income Group`) %>%
  filter(`Indicator Code` == "NY.GDP.PCAP.PP.CD") %>%
  summarise(`1991` = mean(`1991`, na.rm=TRUE),`1992` = mean(`1992`, na.rm=TRUE),
            `1993` = mean(`1993`, na.rm=TRUE),`1994` = mean(`1994`, na.rm=TRUE),
            `1995` = mean(`1995`, na.rm=TRUE),`1996` = mean(`1996`, na.rm=TRUE),
            `1997` = mean(`1997`, na.rm=TRUE),`1998` = mean(`1998`, na.rm=TRUE),
            `1999` = mean(`1999`, na.rm=TRUE),`2000` = mean(`2000`, na.rm=TRUE),
            `2001` = mean(`2001`, na.rm=TRUE),`2002` = mean(`2002`, na.rm=TRUE),
            `2003` = mean(`2003`, na.rm=TRUE),`2004` = mean(`2004`, na.rm=TRUE),
            `2005` = mean(`2005`, na.rm=TRUE),`2006` = mean(`2006`, na.rm=TRUE),
            `2007` = mean(`2007`, na.rm=TRUE),`2008` = mean(`2008`, na.rm=TRUE),
            `2009` = mean(`2009`, na.rm=TRUE),`2010` = mean(`2010`, na.rm=TRUE),
            `2011` = mean(`2011`, na.rm=TRUE),`2012` = mean(`2012`, na.rm=TRUE),
            `2013` = mean(`2013`, na.rm=TRUE),`2014` = mean(`2014`, na.rm=TRUE),
            `2015` = mean(`2015`, na.rm=TRUE),`2016` = mean(`2016`, na.rm=TRUE),
            `2017` = mean(`2017`, na.rm=TRUE),`2018` = mean(`2018`, na.rm=TRUE),
            `2019` = mean(`2019`, na.rm=TRUE))

# impute the null values with averages  
within_model_data_imputed <- within_model_data
for(row in 4:nrow(within_model_data_imputed)) {
  income_group <- as.character(within_model_data_imputed[row,2])
  for (col in 4:32) {
    if (is.na(within_model_data_imputed[row,col])) {
      within_model_data_imputed[row,col] <-
        (average_GDP_stats %>% filter(`Income Group` == income_group))[1,col-2]
    }
    col = col + 1
  }
  row = row + 4
}
```


```{r, echo=FALSE, include=FALSE,cache=TRUE}
library(tidyverse)
GDP_yearly_average <- average_GDP_stats %>% 
  pivot_longer(-`Income Group`, names_to = "year", values_to = "gdp")
GDP_yearly_average$year <- as.numeric(GDP_yearly_average$year)

GDP_yearly_average %>% 
  ggplot(aes(x = year, y = gdp, colour = `Income Group`)) +
  geom_point()

```

```{r, cache = TRUE, include=FALSE}
num_countries <- dim(within_model_data_imputed %>% group_by(`Short Name`) %>%
  summarise(n()))[1]
num_years <- 2019-1991 + 1
num_features <- 4 # add the year

# set a random seed for reproducability
set.seed(123)


within_model_data_imputed <- 
  within_model_data_imputed %>% arrange(`Short Name`, `Indicator Code`)
# GDP, AGR, IND, SRV

# put data into 3d array
data_3d <- array(dim = c(num_countries, num_years, num_features+1)) # add year
for (row in 1:nrow(within_model_data_imputed)) {
  #print(row)
  feature <- row %% num_features 
  if (feature == 0) {
    feature <- num_features 
  }
  #print(feature)
  country = trunc((row+(num_features-1))/num_features)
  for (col_year in 1:num_years) {
    data_3d[country, col_year, feature] <- 
      within_model_data_imputed[row, col_year+3] %>%
      as.numeric()
  }
}

# add year as a variable
for (year in 1:num_years) {
  data_3d[,year,5] <- year+1990
}

```


```{r, cache = TRUE, include=FALSE}
# cut into sections
# set some parameters for our model
max_len <- 4 # the number of previous examples we'll look at
stride <- 1 # striding between segments when selecting time series data
segs_per_country <- (num_years - max_len)/stride

# get a list of start indexes for our (overlapping) chunks
data_segmented <- array(dim = c(num_countries*segs_per_country, 
                                max_len+1, num_features+1)) # add year
for(row in 1:(dim(data_3d)[1])) {
  for(starting_col in 1:segs_per_country) {
    seg_row <- ((row-1)*segs_per_country) + starting_col
    data_segmented[seg_row,,] <- data_3d[row,starting_col:(starting_col+max_len),]
  }
}
```

```{r, cache = TRUE, include=FALSE}
# split into test and train
X_RNN_continuous_GDP <- data_segmented[,,-1]
y_RNN_continuous_GDP <- data_segmented[,,1]

set.seed(43)
train_indices <- sample(1:(dim(data_segmented)[1]), dim(data_segmented)[1]*.8)

# training data
X_train_RNN_continuous_GDP <- X_RNN_continuous_GDP[train_indices,,]
y_train_RNN_continuous_GDP <- y_RNN_continuous_GDP[train_indices,]

# testing data
X_test_RNN_continuous_GDP <- X_RNN_continuous_GDP[-train_indices,,]
y_test_RNN_continuous_GDP <- y_RNN_continuous_GDP[-train_indices,]


```

```{r, include=FALSE, cache=TRUE}
batch_size <- 64 # number of sequences to look at at one time during training
total_epochs <- 100 # how many times we'll look @ the whole dataset while training our model

model_part_1 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_train_RNN_continuous_GDP)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 64, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32, name="RNN_2",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32, name="RNN_3",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

model_part_1 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit
```

```{r, cache = TRUE, include=FALSE, eval=FALSE}
# Actually train our model! This step will take a while
model_part_1 %>% fit(
    x = X_train_RNN_continuous_GDP, # sequence we're using for prediction 
    y = y_train_RNN_continuous_GDP, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.33) # how much data to hold out for testing as we go along

save_model_hdf5(model_part_1, "model_part_1.h5")

```
```{r, cache = TRUE, include=FALSE}
# feature codes
# outcome
vars1 <- c("NY.GDP.PCAP.CD") #"GDP per capita (current US$)"
# agriculture
vars2 <- c("NV.AGR.TOTL.ZS", # Agriculture, forestry, and fishing, value added (% of GDP)
           "SL.AGR.EMPL.ZS", # Employment in agriculture (% of total employment) 
           "NV.AGR.EMPL.KD", # Agriculture, forestry, and fishing, value added per worker (constant 2015 US$)
           "NY.GDP.TOTL.RT.ZS") # Total natural resources rents (% of GDP)
# industry
vars3 <- c("NV.IND.TOTL.ZS", # Industry (including construction), value added (% of GDP)
           "NV.MNF.TECH.ZS.UN", # Medium and high-tech manufacturing value added (% manufacturing value added)
           "SL.IND.EMPL.ZS") # Employment in industry (% of total employment) 
# services
vars4 <- c("NV.SRV.TOTL.ZS", # Services, value added (% of GDP)
           "SL.SRV.EMPL.ZS") # Employment in services (% of total employment) 
# trade
vars5 <- c("NE.EXP.GNFS.ZS", # Exports of goods and services (% of GDP)
           "BG.GSR.NFSV.GD.ZS", # Employment in agriculture (% of total employment) 
           "NE.IMP.GNFS.ZS") # Imports of goods and services (% of GDP)
# macroeconomics
vars6 <- c("FP.CPI.TOTL.ZG", # Inflation, consumer prices (annual %)
           "FS.AST.PRVT.GD.ZS", # Domestic credit to private sector (% of GDP)
           "DT.ODA.ODAT.PC.ZS", # Net ODA received per capita (current US$)
           "NE.GDI.TOTL.ZS", # Gross capital formation (% of GDP)
           "SI.POV.UMIC", # Poverty headcount ratio at $5.50 a day (2011 PPP) (% of population)
           "SI.POV.GINI") # Gini index (World Bank estimate)
# human capital
vars7 <- c("SP.URB.TOTL.IN.ZS", # Urban population (% of total population)
           "SP.POP.1564.TO.ZS", # Population ages 15-64 (% of total population)
           "SL.TLF.CACT.ZS", # Labor force participation rate, total (% of total population ages 15+)
           "SL.TLF.CACT.FM.ZS", # Ratio of female to male labor force participation rate (%)
           "SE.PRM.ENRR", # School enrollment, primary (% gross)
           "SE.PRM.ENRL.FE.ZS", # Primary education, pupils (% female)
           "SE.TER.ENRR", # School enrollment, tertiary (% gross)
           "SE.SEC.ENRL.GC.FE.ZS" # Secondary education, general pupils (% female)
           # ,"SE.SEC.ENRL.VO" # Secondary education, vocational pupils
           ) 
indicators <- c(vars1,vars2,vars3,vars4,vars5,vars6,vars7)
# conflict
conflict_deaths <- conflict_df %>% group_by(country, year) %>%
  summarise(yearly_conflict_deaths = (sum(best))^.5) %>% # raised to .5 to reflect diminishing marginal negative effect as the conflict scales
  ungroup() %>% 
  pivot_wider(names_from = year, values_from = yearly_conflict_deaths) %>%
  ungroup()
conflict_deaths[is.na(conflict_deaths)] = 0
conflict_deaths <- conflict_deaths %>% mutate(
  `Indicator Code` = "Conflict",
  `Indicator Name` = "Armed conflict deaths"
)
conflict_deaths <- conflict_deaths %>% mutate(`Country Name` = 
  case_when(country == "Bosnia-Herzegovina" ~ "Bosnia and Herzegovina",
  country == "Cambodia (Kampuchea)" ~ "Cambodia",
  country == "DR Congo (Zaire)" ~ "Congo, Dem. Rep.",
  country == "Gambia" ~ "Gambia, The",
  country == "Ivory Coast" ~ "Côte d'Ivoire",
  country == "Kingdom of eSwatini (Swaziland)" ~ "Eswatini",
  country == "Kyrgyzstan" ~ "Kyrgyz Republic",
  country == "Laos" ~ "Lao PDR",
  country == "Macedonia, FYR" ~ "North Macedonia",
  country == "Madagascar (Malagasy)" ~ "Madagascar",
  country == "Myanmar (Burma)" ~ "Myanmar",
  country == "Russia (Soviet Union)" ~ "Russia",
  country == "Serbia (Yugoslavia)" ~ "Serbia",
  country == "Syria" ~ "Syrian Arab Republic",
  country == "United States of America" ~ "United States",
  country == "Venezuela" ~ "Venezuela",
  country == "Yemen (North Yemen)" ~ "Yemen",
  country == "Zimbabwe (Rhodesia)" ~ "Zimbabwe",
  TRUE ~ country))
conflict_deaths <- conflict_deaths %>% left_join(
  (wdi_country %>%
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`)),
  by = c("Country Name" = "Short Name")
)
venezuela <- tribble( # manually create income group for Venezuela
  ~country, ~group,
  "Venezuela", "Upper middle income"
)
conflict_deaths <- 
  conflict_deaths %>% 
  left_join(venezuela,
            by = c("Country Name" = "country", "Income Group" = "group"))

# extract final data set to use
indicators_complete <- 
  wdi_raw %>% filter(`Indicator Code` %in% indicators) %>%
  inner_join( 
    (wdi_country %>%
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`)),
  by = c("Country Name" = "Short Name")
  )

indicators_complete <- indicators_complete %>% 
  select(-(`1960`:`1983`), -`...66`, -`Indicator Name`)
conflict_deaths <- conflict_deaths %>%
  mutate(
    `1984` = NA,
    `1985` = NA,
    `1986` = NA,
    `1987` = NA,
    `1988` = NA
  ) %>% select(-country)
indicators_complete <- indicators_complete %>% 
  relocate(where(is.numeric), .after = where(is.character)) %>%
  select(-`Country Code`)
conflict_deaths <- conflict_deaths %>% 
  relocate(where(is.numeric), .after = where(is.character)) %>%
  select(-`Indicator Name`)
indicators_complete_joined <- rbind(indicators_complete, conflict_deaths) %>%
  filter(!(`Country Name` %in% c("Congo", "Côte d'Ivoire", "Dem. Rep. Congo", 
                                 "Egypt", "Iran", "Russia", "The Gambia", "Venezuela", "Yemen",
                                 "Gambia, The", "Congo, Dem. Rep.")))
# growth metric = difference between year 0 and year n
# other growth metric = avg growth in period 
zero_conflict <- indicators_complete_joined %>% group_by(`Country Name`, `Income Group`) %>%
  summarise(count = n()) %>% ungroup() %>% filter(count == 27) %>% select(-count)

# manually add empty conflict row
zero_conflict <- zero_conflict %>% mutate(`Indicator Code` = "Conflict")
zero_matrix <- as_tibble(matrix(0, nrow = dim(zero_conflict)[1], ncol = (dim(indicators_complete_joined)[2]-3)))
year_names <- colnames(indicators_complete_joined %>% select(`1984`:`2020`))
for (i in 1:(dim(indicators_complete_joined)[2]-3)) {
  names(zero_matrix)[i] <- year_names[i]
}
indicators_complete_joined <- rbind(
  cbind(
  zero_conflict %>% mutate(`Indicator Code` = "Conflict"),
  zero_matrix), 
indicators_complete_joined
)
```


```{r, cache = TRUE, include=FALSE}
# value imputation
indicators_imputed <- indicators_complete_joined
for (row in 1:(dim(indicators_complete_joined)[1])) {
  indicator = indicators_complete_joined[row,3] %>% as.character()
  income_group = indicators_complete_joined[row,2] %>% as.character()
  country_avg <- mean(indicators_complete_joined[row,(4:(dim(indicators_complete_joined)[2]))] %>%
                        unlist(use.names=FALSE),
           na.rm = TRUE)
  if (row %% 100 == 0) {
    print(row)
  }
  for (col in 1:(dim(indicators_complete_joined)[2]-3)) {
    val = indicators_complete_joined[row,(dim(indicators_complete_joined)[2] - col+1)] %>%
      as.numeric()
    if (is.na(val)) {
      year <- as.character(dim(indicators_complete_joined)[2] - col+1 +1980)
      year_avg <- mean(indicators_complete_joined %>% # year avg
        filter(
          `Income Group` == income_group,
          `Indicator Code` == indicator
        ) %>%
        pull(year), na.rm=TRUE)
      
    val <- mean(c(year_avg, country_avg), na.rm=TRUE)
    indicators_imputed[row,(dim(indicators_complete_joined)[2] - col+1)] <- val
    }
  }
}

# run twice to really get rid of all NaNs
for (row in 1:(dim(indicators_imputed)[1])) {
  indicator = indicators_imputed[row,3] %>% as.character()
  income_group = indicators_imputed[row,2] %>% as.character()
  country_avg <- mean(indicators_imputed[row,(4:(dim(indicators_imputed)[2]))] %>%
                        unlist(use.names=FALSE),
           na.rm = TRUE)
  if (row %% 100 == 0) {
    print(row)
  }
  for (col in 1:(dim(indicators_imputed)[2]-3)) {
    val = indicators_imputed[row,(dim(indicators_imputed)[2] - col+1)] %>%
      as.numeric()
    #print(val)
    if (is.na(val)) {
      year <- as.character(dim(indicators_imputed)[2] - col+1 +1980)
      year_avg <- mean(indicators_imputed %>% # year avg
        filter(
          `Income Group` == income_group,
          `Indicator Code` == indicator
        ) %>%
        pull(year), na.rm=TRUE)
      
    val <- mean(c(year_avg, country_avg), na.rm=TRUE)
    indicators_imputed[row,(dim(indicators_imputed)[2] - col+1)] <- val
    }
  }
}

indicators_imputed <- indicators_imputed %>% drop_na() # drop na country names

```

```{r, cache = TRUE, include=FALSE}
# Wrangle into correct format
indicators_plm <- indicators_imputed %>%
  pivot_longer(cols = (`1984`:`2020`), names_to = "Year", values_to = "Value") %>%
  pivot_wider(names_from="Indicator Code", values_from = "Value") %>%
  select(-`Income Group`)
indicators_plm$Year = as.numeric(indicators_plm$Year)
indicators_plm$`1yr_growth` <- NA
indicators_plm$`3yr_growth` <- NA
indicators_plm$`5yr_growth` <- NA
indicators_plm$`10yr_growth` <- NA

obs_country <- indicators_plm %>% group_by(`Country Name`) %>%
  summarize(count = n()) %>% head(1) %>% pull(count)
num_countries <- dim(indicators_plm %>% group_by(`Country Name`) %>%
  summarize(count = n()))[1]
gdp_vals <- indicators_plm %>% select(NY.GDP.PCAP.CD) %>% unlist()

country_as_id <- indicators_plm %>% group_by(`Country Name`) %>%
  summarise(country_id = n()) %>% ungroup()
country_as_id$country_id = c(1:(dim(country_as_id)[1]))
indicators_plm <- indicators_plm %>% 
  inner_join(country_as_id, by = c("Country Name" = "Country Name"))

# fill in for train
for (row in 1:(dim(indicators_plm)[1])) {
  num_years_history <- (row - 1) %% obs_country
  this_year <- gdp_vals[row]
  # fill 1 year
  if (num_years_history > 0) {
    yr0 <- gdp_vals[row - 1]
    indicators_plm$`1yr_growth`[row] = (this_year-yr0)/yr0
  }
  # fill 3 year
  if (num_years_history > 2) {
    yr0 <- gdp_vals[row - 3]
    indicators_plm$`3yr_growth`[row] = (this_year-yr0)/yr0
  }
  # fill 5 year
  if (num_years_history > 4) {
    yr0 <- gdp_vals[row - 5]
    indicators_plm$`5yr_growth`[row] = (this_year-yr0)/yr0
  }
  # fill 10 year
  if (num_years_history > 9) {
    yr0 <- gdp_vals[row - 10]
    indicators_plm$`10yr_growth`[row] = (this_year-yr0)/yr0
  }
}
```

```{r, cache = TRUE, include=FALSE}
# tune for number of lags and key variables
max_lags <- 8
num_bootstrap_iterations <- 12
gdp_growth_lags_grid = expand.grid(
  lags = c(1:max_lags),
  test_mse_1yr = 0,
  num_sig_features_1yr = 0,
  num_sig_lags_1yr = 0,
  test_mse_3yr = 0,
  num_sig_features_3yr = 0,
  num_sig_lags_3yr = 0,
  test_mse_5yr = 0,
  num_sig_features_5yr = 0,
  num_sig_lags_5yr = 0,
  test_mse_10yr = 0,
  num_sig_features_10yr = 0,
  num_sig_lags_10yr = 0
)
gdp_pcap_lags_grid <- gdp_growth_lags_grid
gdp_both_lags_grid <- gdp_growth_lags_grid

growth_bootstrap_grid = expand.grid(
  iteration = c(1:num_bootstrap_iterations),
  test_mse_1yr = 0,
  num_sig_features_1yr = 0,
  num_sig_lags_1yr = 0,
  test_mse_3yr = 0,
  num_sig_features_3yr = 0,
  num_sig_lags_3yr = 0,
  test_mse_5yr = 0,
  num_sig_features_5yr = 0,
  num_sig_lags_5yr = 0,
  test_mse_10yr = 0,
  num_sig_features_10yr = 0,
  num_sig_lags_10yr = 0
)
pcap_bootstrap_grid <- growth_bootstrap_grid
both_bootstrap_grid <- growth_bootstrap_grid

zero_count_var_tibble <- tibble(variable = c(colnames(indicators_plm)[3:10], 
                             colnames(indicators_plm)[12:30]),
                             values = c(0))
times_var_important <- zero_count_var_tibble %>% 
  pivot_wider(names_from = "variable", values_from = "values")

get_lags <- function(response, limit) {
  str <- ""
  if (limit > 0) {
    
    for (index in 1:limit) {
    to_add = paste(" +lag(", response, ",", as.character(index), ")")
    str <- paste(str, to_add)
  }
  }
  str
}
all_features <- "0 + NV.AGR.TOTL.ZS + NV.AGR.EMPL.KD + FS.AST.PRVT.GD.ZS + SL.AGR.EMPL.ZS  +  SL.IND.EMPL.ZS  +  SL.SRV.EMPL.ZS +   NE.EXP.GNFS.ZS   +  SI.POV.GINI   +    NE.GDI.TOTL.ZS  +    NE.IMP.GNFS.ZS  +    NV.IND.TOTL.ZS  +    FP.CPI.TOTL.ZG +   SL.TLF.CACT.ZS  +     NV.MNF.TECH.ZS.UN  +     DT.ODA.ODAT.PC.ZS  +     SP.POP.1564.TO.ZS  +   SI.POV.UMIC  +     SE.PRM.ENRL.FE.ZS  +     SL.TLF.CACT.FM.ZS  +     SE.PRM.ENRR  +     SE.TER.ENRR   +   SE.SEC.ENRL.GC.FE.ZS +    NV.SRV.TOTL.ZS  +     NY.GDP.TOTL.RT.ZS  +   BG.GSR.NFSV.GD.ZS  +     SP.URB.TOTL.IN.ZS  +      Conflict"

get_1yr_response <- function(test_data, lags) {
  no_na <- test_data %>% 
    select(-`3yr_growth`, -`5yr_growth`, -`10yr_growth`) %>%
  drop_na()
  min_year <- no_na %>% arrange(Year) %>% head(1) %>% pull(Year)
  test_data %>% filter(Year > (min_year + lags)) %>% pull(`1yr_growth`)
}
get_3yr_response <- function(test_data,lags) {
  no_na <- test_data %>% 
    select(-`1yr_growth`, -`5yr_growth`, -`10yr_growth`)  %>%
    drop_na()
  min_year <- no_na %>% arrange(Year) %>% head(1) %>% pull(Year)
  test_data %>% filter(Year > (min_year + lags)) %>% pull(`3yr_growth`)
}
get_5yr_response <- function(test_data, lags) {
  no_na <- test_data %>% 
    select(-`3yr_growth`, -`1yr_growth`, -`10yr_growth`) %>%
    drop_na()
  min_year <- no_na %>% arrange(Year) %>% head(1) %>% pull(Year)
  test_data %>% filter(Year > (min_year + lags)) %>% pull(`5yr_growth`)
}
get_10yr_response <- function(test_data, lags) {
  no_na <- test_data %>% 
    select(-`3yr_growth`, -`5yr_growth`, -`1yr_growth`) %>%
    drop_na()
  min_year <- no_na %>% arrange(Year) %>% head(1) %>% pull(Year)
  test_data %>% filter(Year > (min_year + lags)) %>% pull(`10yr_growth`)
}

for (i in 0:max_lags) {
  print("lag number")
  print(i)
  # calls
  call_1yr_growth <- paste("X1yr_growth ~ ", all_features, get_lags("X1yr_growth", i))
  call_3yr_growth <- paste("X3yr_growth ~ ", all_features, get_lags("X3yr_growth", i))
  call_5yr_growth <- paste("X5yr_growth ~ ", all_features, get_lags("X5yr_growth", i))
  call_10yr_growth <- paste("X10yr_growth ~ ", all_features, get_lags("X10yr_growth", i))
  
  call_1yr_pcap <- paste("X1yr_growth ~ ", all_features, get_lags("NY.GDP.PCAP.CD", i))
  call_3yr_pcap <- paste("X3yr_growth ~ ", all_features, get_lags("NY.GDP.PCAP.CD", i))
  call_5yr_pcap <- paste("X5yr_growth ~ ", all_features, get_lags("NY.GDP.PCAP.CD", i))
  call_10yr_pcap <- paste("X10yr_growth ~ ", all_features, get_lags("NY.GDP.PCAP.CD", i))
  
  call_1yr_both <- paste("X1yr_growth ~ ", all_features, get_lags("X1yr_growth", i), get_lags("NY.GDP.PCAP.CD", i))
  call_3yr_both <- paste("X3yr_growth ~ ", all_features, get_lags("X3yr_growth", i), get_lags("NY.GDP.PCAP.CD", i))
  call_5yr_both <- paste("X5yr_growth ~ ", all_features, get_lags("X5yr_growth", i), get_lags("NY.GDP.PCAP.CD", i))
  call_10yr_both <- paste("X10yr_growth ~ ", all_features, get_lags("X10yr_growth", i), get_lags("NY.GDP.PCAP.CD", i))
  
  for (iteration in 1:num_bootstrap_iterations) {
    
    # split into train and test
    set.seed(iteration*3)
    train_countries <- sample(1:num_countries, .8*num_countries)
      indicators_plm_train <- indicators_plm %>%
    filter(country_id %in% train_countries)
      indicators_plm_test <- indicators_plm %>%
    filter(!(country_id %in% train_countries))
      
      pseries_train <- pseriesfy(pdata.frame(as.data.frame(indicators_plm_train), 
                         index = c("country_id", "Year")))
      pseries_test <- pseriesfy(pdata.frame(as.data.frame(indicators_plm_test), 
                         index = c("country_id", "Year")))


    # models
  model_1yr_growth <- plm(call_1yr_growth, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_1yr_growth_coefs <- summary(model_1yr_growth)$coefficients
  model_1yr_growth_tibble <- as_tibble(model_1yr_growth_coefs) %>%
    mutate(variable = rownames(model_1yr_growth_coefs))
  model_3yr_growth <- plm(call_3yr_growth, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_3yr_growth_coefs <- summary(model_3yr_growth)$coefficients
  model_3yr_growth_tibble <- as_tibble(model_3yr_growth_coefs) %>%
    mutate(variable = rownames(model_3yr_growth_coefs))
  model_5yr_growth <- plm(call_5yr_growth, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_5yr_growth_coefs <- summary(model_5yr_growth)$coefficients
  model_5yr_growth_tibble <- as_tibble(model_5yr_growth_coefs) %>%
    mutate(variable = rownames(model_5yr_growth_coefs))
  model_10yr_growth <- plm(call_10yr_growth, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_10yr_growth_coefs <- summary(model_10yr_growth)$coefficients
  model_10yr_growth_tibble <- as_tibble(model_10yr_growth_coefs) %>%
    mutate(variable = rownames(model_10yr_growth_coefs))
  
  model_1yr_pcap <- plm(call_1yr_pcap, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_1yr_pcap_coefs <- summary(model_1yr_pcap)$coefficients
  model_1yr_pcap_tibble <- as_tibble(model_1yr_pcap_coefs) %>%
    mutate(variable = rownames(model_1yr_pcap_coefs))
  model_3yr_pcap <- plm(call_3yr_pcap, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_3yr_pcap_coefs <- summary(model_3yr_pcap)$coefficients
  model_3yr_pcap_tibble <- as_tibble(model_3yr_pcap_coefs) %>%
    mutate(variable = rownames(model_3yr_pcap_coefs))
  model_5yr_pcap <- plm(call_5yr_pcap, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_5yr_pcap_coefs <- summary(model_5yr_pcap)$coefficients
  model_5yr_pcap_tibble <- as_tibble(model_5yr_pcap_coefs) %>%
    mutate(variable = rownames(model_5yr_pcap_coefs))
  model_10yr_pcap <- plm(call_10yr_pcap, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_10yr_pcap_coefs <- summary(model_10yr_pcap)$coefficients
  model_10yr_pcap_tibble <- as_tibble(model_10yr_pcap_coefs) %>%
    mutate(variable = rownames(model_10yr_pcap_coefs))
  
  model_1yr_both <- plm(call_1yr_both, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_1yr_both_coefs <- summary(model_1yr_both)$coefficients
  model_1yr_both_tibble <- as_tibble(model_1yr_both_coefs) %>%
    mutate(variable = rownames(model_1yr_both_coefs))
  model_3yr_both <- plm(call_3yr_both, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_3yr_both_coefs <- summary(model_3yr_both)$coefficients
  model_3yr_both_tibble <- as_tibble(model_3yr_both_coefs) %>%
    mutate(variable = rownames(model_3yr_both_coefs))
  model_5yr_both <- plm(call_5yr_both, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_5yr_both_coefs <- summary(model_5yr_both)$coefficients
  model_5yr_both_tibble <- as_tibble(model_5yr_both_coefs) %>%
    mutate(variable = rownames(model_5yr_both_coefs))
  model_10yr_both <- plm(call_10yr_both, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_10yr_both_coefs <- summary(model_10yr_both)$coefficients
  model_10yr_both_tibble <- as_tibble(model_10yr_both_coefs) %>%
    mutate(variable = rownames(model_10yr_both_coefs))
  
  # results
  # 1yr response
  resp_1yr <- get_1yr_response(indicators_plm_test, i) 
  growth_bootstrap_grid$test_mse_1yr[iteration] =
    mean((predict(model_1yr_growth,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_1yr)^2, na.rm=TRUE)
  pcap_bootstrap_grid$test_mse_1yr[iteration] =
    mean((predict(model_1yr_pcap,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_1yr)^2, na.rm=TRUE)
  both_bootstrap_grid$test_mse_1yr[iteration] = 
    mean((predict(model_1yr_both,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_1yr)^2, na.rm=TRUE)
  
  # 3yr response
  resp_3yr <- get_3yr_response(indicators_plm_test, i)
  growth_bootstrap_grid$test_mse_3yr[iteration] =
    mean((predict(model_3yr_growth,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_3yr)^2, na.rm=TRUE)
  pcap_bootstrap_grid$test_mse_3yr[iteration] =
    mean((predict(model_3yr_pcap,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_3yr)^2, na.rm=TRUE)
  both_bootstrap_grid$test_mse_3yr[iteration] = 
    mean((predict(model_3yr_both,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_3yr)^2, na.rm=TRUE)
  # 5 yr response
  resp_5yr <- get_5yr_response(indicators_plm_test, i)
  growth_bootstrap_grid$test_mse_5yr[iteration] =
    mean((predict(model_5yr_growth,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_5yr)^2, na.rm=TRUE)
  pcap_bootstrap_grid$test_mse_5yr[iteration] =
    mean((predict(model_5yr_pcap,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_5yr)^2, na.rm=TRUE)
  both_bootstrap_grid$test_mse_5yr[iteration] = 
    mean((predict(model_5yr_both,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_5yr)^2, na.rm=TRUE)
  # 10 yr response
  resp_10yr <- get_10yr_response(indicators_plm_test, i)
  growth_bootstrap_grid$test_mse_10yr[iteration] =
    mean((predict(model_10yr_growth,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_10yr)^2, na.rm=TRUE)
  pcap_bootstrap_grid$test_mse_10yr[iteration] =
    mean((predict(model_10yr_pcap,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_10yr)^2, na.rm=TRUE)
  both_bootstrap_grid$test_mse_10yr[iteration] = 
    mean((predict(model_10yr_both,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_10yr)^2, na.rm=TRUE)
  
  
  
  # extract number significant features
  growth_bootstrap_grid$num_sig_features_1yr[iteration] =
    model_1yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_features_3yr[iteration] =
    model_3yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_features_5yr[iteration] =
    model_5yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_features_10yr[iteration] =
    model_10yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  
  pcap_bootstrap_grid$num_sig_features_1yr[iteration] =
    model_1yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_features_3yr[iteration] =
    model_3yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_features_5yr[iteration] =
    model_5yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_features_10yr[iteration] =
    model_10yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  
  both_bootstrap_grid$num_sig_features_1yr[iteration] =
    model_1yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  both_bootstrap_grid$num_sig_features_3yr[iteration] =
    model_3yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  both_bootstrap_grid$num_sig_features_5yr[iteration] =
    model_5yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  both_bootstrap_grid$num_sig_features_10yr[iteration] =
    model_10yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  
  # extract number significant lags
  growth_bootstrap_grid$num_sig_lags_1yr[iteration] =
    model_1yr_growth_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_lags_3yr[iteration] =
    model_3yr_growth_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_lags_5yr[iteration] =
    model_5yr_growth_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_lags_10yr[iteration] =
    model_10yr_growth_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  
  pcap_bootstrap_grid$num_sig_lags_1yr[iteration] =
    model_1yr_pcap_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_lags_3yr[iteration] =
    model_3yr_pcap_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_lags_5yr[iteration] =
    model_5yr_pcap_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_lags_10yr[iteration] =
    model_10yr_pcap_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  
  both_bootstrap_grid$num_sig_lags_1yr[iteration] =
    model_1yr_both_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  both_bootstrap_grid$num_sig_lags_3yr[iteration] =
    model_3yr_both_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  both_bootstrap_grid$num_sig_lags_5yr[iteration] =
    model_5yr_both_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
 both_bootstrap_grid$num_sig_lags_10yr[iteration] =
    model_10yr_both_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
 
 
 # count # times significant each variable
  this_round_sig_vars <- bind_rows(
    model_1yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable),
    model_3yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable),
    model_5yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable),
    model_10yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable),
    
    model_1yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable),
    model_3yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable),
    model_5yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable),
    model_10yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable),
    
    model_1yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable),
    model_3yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable),
    model_5yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable),
    model_10yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.05 | `Pr(>|t|)` > 0.95) %>% select(variable)
  ) %>% group_by(variable) %>% summarise(count = n()) %>% ungroup() %>%
    right_join(zero_count_var_tibble, by = c("variable" = "variable")) %>%
    mutate(round_count = replace_na(count, 0)) %>% select(-values, -count) %>%
    pivot_wider(names_from = "variable", values_from = "round_count")
  times_var_important <- bind_rows(times_var_important, this_round_sig_vars)
  
  }
  
  gdp_growth_lags_grid$test_mse_1yr[i] = mean(growth_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_features_1yr[i] = mean(growth_bootstrap_grid$num_sig_features_1yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_lags_1yr[i] = mean(growth_bootstrap_grid$num_sig_lags_1yr, na.rm=TRUE)
  gdp_growth_lags_grid$test_mse_3yr[i] = mean(growth_bootstrap_grid$test_mse_3yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_features_3yr[i] = mean(growth_bootstrap_grid$num_sig_features_3yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_lags_3yr[i] = mean(growth_bootstrap_grid$num_sig_lags_3yr, na.rm=TRUE)
  gdp_growth_lags_grid$test_mse_5yr[i] = mean(growth_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_features_5yr[i] = mean(growth_bootstrap_grid$num_sig_features_5yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_lags_5yr[i] = mean(growth_bootstrap_grid$num_sig_lags_5yr, na.rm=TRUE)
  gdp_growth_lags_grid$test_mse_10yr[i] = mean(growth_bootstrap_grid$test_mse_10yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_features_10yr[i] = mean(growth_bootstrap_grid$num_sig_features_10yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_lags_10yr[i] = mean(growth_bootstrap_grid$num_sig_lags_10yr, na.rm=TRUE)
  
  gdp_pcap_lags_grid$test_mse_1yr[i] = mean(pcap_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_features_1yr[i] = mean(pcap_bootstrap_grid$num_sig_features_1yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_lags_1yr[i] = mean(pcap_bootstrap_grid$num_sig_lags_1yr, na.rm=TRUE)
  gdp_pcap_lags_grid$test_mse_3yr[i] = mean(pcap_bootstrap_grid$test_mse_3yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_features_3yr[i] = mean(pcap_bootstrap_grid$num_sig_features_3yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_lags_3yr[i] = mean(pcap_bootstrap_grid$num_sig_lags_3yr, na.rm=TRUE)
  gdp_pcap_lags_grid$test_mse_5yr[i] = mean(pcap_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_features_5yr[i] = mean(pcap_bootstrap_grid$num_sig_features_5yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_lags_5yr[i] = mean(pcap_bootstrap_grid$num_sig_lags_5yr, na.rm=TRUE)
  gdp_pcap_lags_grid$test_mse_10yr[i] = mean(pcap_bootstrap_grid$test_mse_10yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_features_10yr[i] = mean(pcap_bootstrap_grid$num_sig_features_10yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_lags_10yr[i] = mean(pcap_bootstrap_grid$num_sig_lags_10yr, na.rm=TRUE)

  
  gdp_both_lags_grid$test_mse_1yr[i] = mean(both_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_features_1yr[i] = mean(both_bootstrap_grid$num_sig_features_1yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_lags_1yr[i] = mean(both_bootstrap_grid$num_sig_lags_1yr, na.rm=TRUE)
  gdp_both_lags_grid$test_mse_3yr[i] = mean(both_bootstrap_grid$test_mse_3yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_features_3yr[i] = mean(both_bootstrap_grid$num_sig_features_3yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_lags_3yr[i] = mean(both_bootstrap_grid$num_sig_lags_3yr, na.rm=TRUE)
  gdp_both_lags_grid$test_mse_5yr[i] = mean(both_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_features_5yr[i] = mean(both_bootstrap_grid$num_sig_features_5yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_lags_5yr[i] = mean(both_bootstrap_grid$num_sig_lags_5yr, na.rm=TRUE)
  gdp_both_lags_grid$test_mse_10yr[i] = mean(both_bootstrap_grid$test_mse_10yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_features_10yr[i] = mean(both_bootstrap_grid$num_sig_features_10yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_lags_10yr[i] = mean(both_bootstrap_grid$num_sig_lags_10yr, na.rm=TRUE)
}

FINAL_times_plm_coef_sig <- colSums(times_var_important)
```
```{r, cache = TRUE, include=FALSE}
gdp_growth_lags_grid <- as_tibble(gdp_growth_lags_grid) %>%
  mutate(lag_type = "growth")
gdp_pcap_lags_grid <- as_tibble(gdp_pcap_lags_grid) %>%
  mutate(lag_type = "gdp_pcap")
gdp_both_lags_grid <- as_tibble(gdp_both_lags_grid) %>%
  mutate(lag_type = "both")

param_grid_combined = rbind(
  gdp_growth_lags_grid,
  gdp_pcap_lags_grid,
  gdp_both_lags_grid
)

param_grid_combined$scaled_1yr <- scale(param_grid_combined$test_mse_1yr)
param_grid_combined$scaled_3yr <- scale(param_grid_combined$test_mse_3yr)
param_grid_combined$scaled_5yr <- scale(param_grid_combined$test_mse_5yr)
param_grid_combined$scaled_10yr <- scale(param_grid_combined$test_mse_10yr)

param_grid_combined %>% 
  pivot_longer(cols = c(scaled_1yr, scaled_3yr, scaled_5yr, scaled_10yr),
               names_to = "growth_period",
               values_to = "scaled_test_mse") %>%
  ggplot(aes(x = lags, y = scaled_test_mse, 
             shape = growth_period, colour = lag_type)) +
  geom_point() +
  geom_smooth(se = FALSE)

# we see that there is like no trend in the data at all lmao
# bootstrap it?
# take a look at the 
# growth seems to be a much better choice of lags than other kinds of lags 

```
```{r, cache = TRUE, include=FALSE}
# Occam's Razor Principle
best_call <- paste("X3yr_growth ~ ", all_features, get_lags("X3yr_growth", 4))
pseries <- pseriesfy(pdata.frame(as.data.frame(indicators_plm), 
                         index = c("country_id", "Year")))
summary(plm(best_call, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within"))

```

```{r, cache = TRUE, include=FALSE, eval=FALSE}
# wrangle the data into 3 dimensions
to_3d_all_features <- indicators_plm %>% 
  select(-`1yr_growth`, -`5yr_growth`, -`10yr_growth`, -country_id) %>% # select 3 yr growth as response
  pivot_longer(cols=c(Conflict, NV.AGR.TOTL.ZS:`3yr_growth`), names_to = "Indicator Code", values_to = "vals") %>%
  #filter(!(`Indicator Code` == "NY.GDP.PCAP.CD")) %>%
  filter(Year > 1986) %>% # filter out years with null response
  pivot_wider(names_from = Year, values_from = vals) %>%
  arrange(`Country Name`, `Indicator Code`) %>% drop_na() %>%
  filter(!(`Country Name` %in% c("Congo, Dem. Rep.", "Gambia, The")))

to_3d_all_features %>% group_by(`Country Name`) %>%
  summarise(count = n())

num_countries <- dim(to_3d_all_features %>% group_by(`Country Name`) %>%
  summarise(n()))[1]
num_years <- 2020-1987 + 1
num_features <- dim(to_3d_all_features %>% group_by(`Indicator Code`) %>%
  summarise(n()))[1] # add the year

# set a random seed for reproducibility
set.seed(123)

# put data into 3d array
all_features_3d <- array(dim = c(num_countries, num_years, num_features+1)) # add year
for (row in 1:nrow(to_3d_all_features)) {
  #print(row)
  feature <- row %% num_features 
  if (feature == 0) {
    feature <- num_features 
  }
  #print(feature)
  country = trunc((row+(num_features-1))/num_features)
  for (col_year in 1:num_years) {
    all_features_3d[country, col_year, feature] <- 
      to_3d_all_features[row, col_year+2] %>%
      as.numeric()
  }
}

# add year as a variable
for (year in 1:num_years) {
  all_features_3d[,year,num_features+1] <- year+1986
}
```


```{r, cache = TRUE, include=FALSE, eval=FALSE}
# cut into sections
# set some parameters for our model
# max_len ... this time let's try different values
stride <- 1 # striding between segments when selecting time series data
segs_per_country_maxlen3 <- (num_years - 3)/stride
segs_per_country_maxlen4 <- (num_years - 4)/stride
segs_per_country_maxlen5 <- (num_years - 5)/stride

# get a list of start indexes for our (overlapping) chunks
max_len_3_data <- array(dim = c(num_countries*segs_per_country_maxlen3, 
                                3+1, # num of prev value + current value
                                num_features+1)) # add year
max_len_4_data <- array(dim = c(num_countries*segs_per_country_maxlen4, 
                                4+1, # num of prev value + current value
                                num_features+1)) # add year
max_len_5_data <- array(dim = c(num_countries*segs_per_country_maxlen5, 
                                5+1, # num of prev value + current value
                                num_features+1)) # add year

for(row in 1:(dim(all_features_3d)[1])) {
  for(starting_col in 1:segs_per_country_maxlen3) {
    seg_row <- ((row-1)*segs_per_country_maxlen3) + starting_col
    max_len_3_data[seg_row,,] <- all_features_3d[row,starting_col:(starting_col+3),]
  }
}
for(row in 1:(dim(all_features_3d)[1])) {
  for(starting_col in 1:(num_years - 4)) {
    seg_row <- ((row-1)*(num_years - 4)) + starting_col
    max_len_4_data[seg_row,,] <- all_features_3d[row,starting_col:(starting_col+4),]
  }
}
for(row in 1:(dim(all_features_3d)[1])) {
  for(starting_col in 1:(num_years - 5)) {
    seg_row <- ((row-1)*(num_years - 5)) + starting_col
    max_len_5_data[seg_row,,] <- all_features_3d[row,starting_col:(starting_col+5),]
  }
}

```

```{r, cache = TRUE, include=FALSE, eval=FALSE}
# max_len 3 data
# split into test and train
X_RNN_full_data_maxlen3 <- max_len_3_data[,,-1]
y_RNN_full_data_maxlen3 <- max_len_3_data[,,1]

set.seed(43)
train_indices_maxlen3 <- sample(1:(dim(max_len_3_data)[1]), dim(max_len_3_data)[1]*.8)

# training data
X_RNN_full_train_maxlen3 <- X_RNN_full_data_maxlen3[train_indices_maxlen3,,]
y_RNN_full_train_maxlen3 <- y_RNN_full_data_maxlen3[train_indices_maxlen3,]

# testing data
X_RNN_full_test_maxlen3 <- X_RNN_full_data_maxlen3[-train_indices_maxlen3,,]
y_RNN_full_test_maxlen3 <- y_RNN_full_data_maxlen3[-train_indices_maxlen3,]

# max_len 4 data
# split into test and train
X_RNN_full_data_maxlen4 <- max_len_4_data[,,-1]
y_RNN_full_data_maxlen4 <- max_len_4_data[,,1]

set.seed(43)
train_indices_maxlen4 <- sample(1:(dim(max_len_4_data)[1]), dim(max_len_4_data)[1]*.8)

# training data
X_RNN_full_train_maxlen4 <- X_RNN_full_data_maxlen4[train_indices_maxlen4,,]
y_RNN_full_train_maxlen4 <- y_RNN_full_data_maxlen4[train_indices_maxlen4,]

# testing data
X_RNN_full_test_maxlen4 <- X_RNN_full_data_maxlen4[-train_indices_maxlen4,,]
y_RNN_full_test_maxlen4 <- y_RNN_full_data_maxlen4[-train_indices_maxlen4,]


# max_len 5 data
# split into test and train
X_RNN_full_data_maxlen5 <- max_len_5_data[,,-1]
y_RNN_full_data_maxlen5 <- max_len_5_data[,,1]

set.seed(43)
train_indices_maxlen5 <- sample(1:(dim(max_len_5_data)[1]), dim(max_len_5_data)[1]*.8)

# training data
X_RNN_full_train_maxlen5 <- X_RNN_full_data_maxlen5[train_indices_maxlen5,,]
y_RNN_full_train_maxlen5 <- y_RNN_full_data_maxlen5[train_indices_maxlen5,]

# testing data
X_RNN_full_test_maxlen5 <- X_RNN_full_data_maxlen5[-train_indices_maxlen5,,]
y_RNN_full_test_maxlen5 <- y_RNN_full_data_maxlen5[-train_indices_maxlen5,]

```

```{r, cache = TRUE, include=FALSE, eval=FALSE}
batch_size <- 64 # number of sequences to look at at one time during training
total_epochs <- 100 # how many times we'll look @ the whole dataset while training our model

# maxlen 3
# model one
complex_1_maxlen_3 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train_maxlen3)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

complex_1_maxlen_3 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit


# model two
complex_2_maxlen_3 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train_maxlen3)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_2",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

complex_2_maxlen_3 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit

# model three
complex_3_maxlen_3 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train_maxlen3)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_2",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_3",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

complex_3_maxlen_3 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit


# maxlen 4
# model one
complex_1_maxlen_4 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train_maxlen4)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

complex_1_maxlen_4 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit


# model two
complex_2_maxlen_4 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train_maxlen4)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_2",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

complex_2_maxlen_4 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit

# model three
complex_3_maxlen_4 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train_maxlen4)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_2",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_3",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

complex_3_maxlen_4 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit



# maxlen 3
# model one
complex_1_maxlen_5 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train_maxlen5)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

complex_1_maxlen_5 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit


# model two
complex_2_maxlen_5 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train_maxlen5)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_2",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

complex_2_maxlen_5 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit

# model three
complex_3_maxlen_5 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train_maxlen5)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_2",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_3",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

complex_3_maxlen_5 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit
```

```{r, cache = TRUE, include=FALSE, eval=FALSE}
# Actually train our model! This step will take a while
complex_1_maxlen_3 %>% fit(
    x = X_RNN_full_train_maxlen3, # sequence we're using for prediction 
    y = y_RNN_full_train_maxlen3, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along

complex_2_maxlen_3 %>% fit(
    x = X_RNN_full_train_maxlen3, # sequence we're using for prediction 
    y = y_RNN_full_train_maxlen3, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along

complex_3_maxlen_3 %>% fit(
    x = X_RNN_full_train_maxlen3, # sequence we're using for prediction 
    y = y_RNN_full_train_maxlen3, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along


complex_1_maxlen_4 %>% fit(
    x = X_RNN_full_train_maxlen4, # sequence we're using for prediction 
    y = y_RNN_full_train_maxlen4, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along

complex_2_maxlen_4 %>% fit(
    x = X_RNN_full_train_maxlen4, # sequence we're using for prediction 
    y = y_RNN_full_train_maxlen4, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along

complex_3_maxlen_4 %>% fit(
    x = X_RNN_full_train_maxlen4, # sequence we're using for prediction 
    y = y_RNN_full_train_maxlen4, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along

complex_1_maxlen_5 %>% fit(
    x = X_RNN_full_train_maxlen5, # sequence we're using for prediction 
    y = y_RNN_full_train_maxlen5, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along

complex_2_maxlen_5 %>% fit(
    x = X_RNN_full_train_maxlen5, # sequence we're using for prediction 
    y = y_RNN_full_train_maxlen5, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along

complex_3_maxlen_5 %>% fit(
    x = X_RNN_full_train_maxlen5, # sequence we're using for prediction 
    y = y_RNN_full_train_maxlen5, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along

```

```{r, cache = TRUE, include=FALSE, eval=FALSE}
# RNN model comparison
complex1_maxlen3_mean_abs_error <- 
  as.numeric(tensorflow::evaluate(complex_1_maxlen_3, X_RNN_full_test_maxlen3, y_RNN_full_test_maxlen3, verbose = FALSE)[2])
complex2_maxlen3_mean_abs_error <- 
  as.numeric(tensorflow::evaluate(complex_2_maxlen_3, X_RNN_full_test_maxlen3, y_RNN_full_test_maxlen3, verbose = FALSE)[2])
complex3_maxlen3_mean_abs_error <- 
  as.numeric(tensorflow::evaluate(complex_3_maxlen_3, X_RNN_full_test_maxlen3, y_RNN_full_test_maxlen3, verbose = FALSE)[2])

complex1_maxlen4_mean_abs_error <- 
  as.numeric(tensorflow::evaluate(complex_1_maxlen_4, X_RNN_full_test_maxlen4, y_RNN_full_test_maxlen4, verbose = FALSE)[2])
complex2_maxlen4_mean_abs_error <- 
  as.numeric(tensorflow::evaluate(complex_2_maxlen_4, X_RNN_full_test_maxlen4, y_RNN_full_test_maxlen4, verbose = FALSE)[2])
complex3_maxlen4_mean_abs_error <- 
  as.numeric(tensorflow::evaluate(complex_3_maxlen_4, X_RNN_full_test_maxlen4, y_RNN_full_test_maxlen4, verbose = FALSE)[2])

complex1_maxlen5_mean_abs_error <- 
  as.numeric(tensorflow::evaluate(complex_1_maxlen_5, X_RNN_full_test_maxlen5, y_RNN_full_test_maxlen5, verbose = FALSE)[2])
complex2_maxlen5_mean_abs_error <- 
  as.numeric(tensorflow::evaluate(complex_2_maxlen_5, X_RNN_full_test_maxlen5, y_RNN_full_test_maxlen5, verbose = FALSE)[2])
complex3_maxlen5_mean_abs_error <- 
  as.numeric(tensorflow::evaluate(complex_3_maxlen_5, X_RNN_full_test_maxlen5, y_RNN_full_test_maxlen5, verbose = FALSE)[2])

rnn_model_errs_tibble <- 
  tibble(
    complexity = c("One layer", "Two layers", "Three layers"),
    max_len_3 = c(complex1_maxlen3_mean_abs_error, complex2_maxlen3_mean_abs_error, 
                  complex3_maxlen3_mean_abs_error),
    max_len_4 = c(complex1_maxlen4_mean_abs_error, complex2_maxlen4_mean_abs_error,
                  complex3_maxlen4_mean_abs_error),
    max_len_5 = c(complex1_maxlen5_mean_abs_error, complex2_maxlen5_mean_abs_error, 
                  complex3_maxlen5_mean_abs_error)
  )
write.table(rnn_model_errs_tibble, "rnn-model-comparison")
```

```{r, cache = TRUE, include=FALSE, eval=FALSE}
# feature importance throgh scrambling
scrambled_feature_mse_vector <- rep(0, (dim(X_RNN_full_test_maxlen5)[3]-1))
for (feature in 1:(dim(X_RNN_full_test_maxlen5)[3]-1)) {
  scrambled_3d <- X_RNN_full_test_maxlen5
  for (t in 1:(dim(scrambled_3d)[2])) {
    scrambled_3d[,t,feature] <- sample(scrambled_3d[,t,feature],
                                       length(scrambled_3d[,t,feature]),
                                       replace = FALSE)
  }
  scrambled_feature_mse_vector[feature] <-
    as.numeric(tensorflow::evaluate(complex_2_maxlen_5, scrambled_3d, 
                        y_RNN_full_test_maxlen5, verbose = FALSE)[2])
}
min_rnn_err <- complex2_maxlen5_mean_abs_error
feature_vector <- to_3d_all_features %>%
  slice(2:(dim(X_RNN_full_test_maxlen3)[3])) %>%
  pull(`Indicator Code`)
# scrambled_feature_mse_vector
scrambled_feature_tibble <- tibble(feature_number = c(1:(dim(scrambled_3d)[3]-1)),
                                   deviance = scrambled_feature_mse_vector - min_rnn_err,
                                   feature_name = feature_vector) %>%
  arrange(desc(deviance))
write.table(scrambled_feature_tibble, "rnn-feature-scramble")
```

# Data

## Data Sources

The vast majority of the data were collected from the World Bank Development Indicators. The Development Indicators can be accessed [here](https://databank.worldbank.org/source/world-development-indicators). The data contain over 1400 time series indicators for 217 different economies and over 40 supranational groupings, with several indicators going back over 50 years. Because the task of collecting such diffuse data is impossible, for one organization to manage, the World Bank sources a lot of the data from official sources such as national statistics organizations, United Nations agencies, academia and beyond. However, the World Bank Group does conduct several surveys and research projects of their own to add to the data set.

However, since I knew that armed conflict would have a huge influence on the macroeconomic growth of an observation country, I decided to join more data sourced from the [Uppsala Conflict Data Program (UCDP)](https://ucdp.uu.se/?id=1&id=1). The UCDP collects data on armed conflict since 1975. They define an "event" as any instance of fatal organized violence. And to ensure truly global representation in their data set, they utilize a keyword search through the Dow Jones Factiva Indicator, which annually returns around 50 thousand unique events which are then reviewed by human evaluators, resulting in around 10-12 new events coded each year. 

## Data Cleaning

To clean the data, the World Bank Development Indicators were first filtered to exclude non-country observations in the data. For the indicators, since many only have observations for a small select set of (usually wealthy and developed) countries, or only a short temporal range for the time series, or the many missing values in between where data was not collected for whatever reason, I first filtered for indicators that had adequate data for the model. This reduced the number of indicators from 1443 to 497. Then, from this list, I manually chose features which lacked high levels of multicollinearity, were expressed as a percentage rather than in absolute numbers (to avoid selection bias), and were very relevant to the project. Lastly, I removed countries without a certain availability of data and restricted the temporal range to 1984-2020, since the data became less abundant earlier in the set. In order to join the World Bank Development Indicators on the UCDP conflict data, some country names had to be manually adjusted. 

Following this selection of data, values were manually imputed to make sure there were no NaN values in the data set. This imputation was done by iterating through every value in the panel data set, and, if it was null, calculating a "yearly" average and an "observation" average and then averaging both. The yearly average consisted of the average value for that indicator in a given year among all countries in that income group ("High income", "Upper middle income", "Lower middle income", or "Low income"). The observation average was the mean value for a given indicator for a given country across all years in the data set. 

To wrangle the data into a format suitable for the time-series fixed effects regression in [Model 1: Linear Methods (Linear Models for Panel Data)], the data was first transformed into a `panel data dataframe` from the `plm` package, and then subsequently transformed into a `pseries` object from the package with the `Year` and `country_id` as indices. 

For the Recurrent Neural Network models, the data had to be wrangled into a three-dimensional format, where the first dimension represented observations, the second dimension represented time, and the third dimension represented the features of the model. This was done iteratively through for loops, parsing every value in the two-dimensional data and mapping it to its correct position in the three-dimensional array. Following this, each country time series was split into 5 year long strips with a stride of one (e.g. one strip for 1999-2003, one for 2000-2004, one for 2001-2005, etc.) and then aggregated into another 3-dimensional array.

## Data Description

```{r data-description-1, cache=TRUE, eval=TRUE, echo=FALSE}
indicators_imputed <- indicators_imputed %>% drop_na()
num_indicators <- dim(indicators_imputed %>% group_by(`Indicator Code`) %>%
  summarise())[1]
num_income_groups <- 4
num_years <- 2020-1984+1
num_countries <- dim(indicators_imputed %>% group_by(`Country Name`) %>%
  summarise())[1]
num_rows <- dim(indicators_imputed)[1]
num_cols <- dim(indicators_imputed)[2]
tibble(stat = c("Number of Indicators",
                "Number of Income Groups",
                "Number of Years Considered",
                "Number of Countries",
                "Number of Rows",
                "Number of Columns"),
       val = c(num_indicators, num_income_groups, num_years, 
               num_countries, num_rows, num_cols)) %>%
  kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
                        col.names = c("Summary Statistic",
                                      "Value"),
        caption = "Summary Statistics of Data with Value Imputation") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
  
```

Summary statistics of the data can be seen from Table \@ref(tab:data-description-1). There are a total of 5600 observations and 40 columns in the principal data set used, after value imputation. These observations can be divided into time series data (from 1984 to 2020) for 200 countries, grouped into 4 different income groups, across 28 different indicators. 

```{r feature-description, cache=TRUE, eval=TRUE, echo=FALSE}
wdi_series %>% filter(`Series Code` %in% indicators) %>% 
  select(`Series Code`, `Indicator Name`) %>%
  add_row(`Series Code` = "Conflict", 
          `Indicator Name` = "Sqrt of conflict deaths given country and year") %>%
  add_column(Type = c("Continuous")) %>%
  kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
        caption = "List of Indicators Used in Analysis") %>%
  column_spec(2, width="4in") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
  

```

The 28 different features considered in the data set are listed in Table \@ref(tab:feature-description). However, these features should be considered only 27, since the variable `NY.GDP.PCAP.CD` or `GDP per capita (current US$)` was used to construct the response variables. In [Part II: Economic Structure and Economic Growth], the response variable was continuous, and was defined as a the percentage change in GDP of a given observation (country) over a certain period of time. This could be calculated as (GDP in year n - GDP in year 0)/GDP in year 0. The time periods of 1 year, 3 years, 5 years and 10 years were all tried on the data set. 

For [Part I: Economic Structure and Economic Development Level], the continuous feature `NY.GDP.PCAP.CD` was used directly as the response variable. 

## Data Allocation

To get good estimates for [Model 1: Linear Methods (Linear Models for Panel Data)], in this section, the values were bootstrapped and models were calculated from train/test splits multiple times. In each train/test split, 80% of the values were used for training. 20% of the values were used for testing, and test MSE, number of significant features and other metrics were calculated on this test data and aggregated. 

In [Model 2: Recurrent Neural Network], no bootstrapping was done, but 80% of the data was held in reserve as test data to assess model fit. During training, the neural network held 25% of the training data in reserve as validation data in order to tune the parameters. 

# Modeling

## Part I: Economic Structure and Economic Development Level

This first portion of data looked to see at the relationship between the structure of an economy at the most aggregate, generalized level and its economic development level, measured in real (purchasing power parity) GDP per capita. It consists of [Exploratory Data Analysis (Data Exploration)], which looks at summary statistics to understand this relationship and [Model 1: Recurrent Neural Network], which builds a RNN model to predict GDP (PPP) per capita.

### Exploratory Data Analysis (Data Exploration)

```{r plot-grid-p1-p3, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 6, fig.height = 7, out.width = "100%", fig.align='center', fig.cap = "Average Employment by Sector by Income Group Over Time"}
plot_grid(p1,p2,p3,ncol=1)
```

From Figure \@ref(fig:plot-grid-p1-p3), it is very visible that there is a clear correlation between the structure of an economy and its "income level", as classified by the World Bank. As a country gets richer, resources are very clearly taken from agriculture and reallocated to industry and services, with services being an even larger recipient than industry. Furthermore, we see that not only has this trend remains quite constant over time, but that the relative gaps between income groups in various industries also seems quite constant over the past three decades. 

```{r tens-boxplot, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 9, fig.height = 5, out.width = "100%", fig.align='center', fig.cap = "Distribution of Employment by Sector by Income Group 2010-2019"}
tens_boxplot
```


Figure \@ref(fig:tens-boxplot) further affirms the strength of the connection between economic mix and GDP per capita. Not only are the averages so disparate across income groups, but that distributions are also very tight. Outliers in this data set are very few, and even if you look at the outliers with the highest share of agriculture in the "High income group", they all have a lower share of the economy dedicated to agriculture than the median "Upper middle income" country. 


```{r share-over-time-change-plot, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 9, fig.height = 5, out.width = "100%", fig.align='center', fig.cap = "Sector Share Change by Income Group 1991-2019"}
shares_change_over_time %>%
  ggplot(aes(x = year, y = share, colour=indicator)) +
  geom_point() +
  facet_wrap(~`Income Group`) +
  scale_x_discrete(breaks = seq(1991, 2019, by = 5)) +
  labs(xlab = "Year", ylab = "Share of Empoyment") + 
  theme_bw()
```

Figure \@ref(fig:share-over-time-change-plot) shows us the importance of controlling for time in any model. The relative share of the economy dedicated to services has exploded across all income groups. Likewise, the share dedicated to agriculture has fallen through time in all income groups, as well, though the decline was perhaps more precipitous in poorer countries than in richer countries. 

```{r pct-chg-groups-time-tibble, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
pct_change_groups_time %>% kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
                        col.names = c("Income Group",
                                      "Pct Change Agriculture",
                                      "Pct Change Industry",
                                      "Pct Change Services"),
        caption = "1991-2019 Pct Change in Share of Economy Dedicated to a Sector by Income Group") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
  
```

Table \@ref(tab:pct-chg-groups-time-tibble) again shows that the economies are not static. Subject to market forces, they have been evolving over time in an enormous way (think of how much time and resources it takes to train a smallholder farmer to be a teacher, bus driver, programmer, banker, or another professional in the service industry). Table \@ref(tab:pct-chg-groups-time-tibble) shows that changes to the percentage of the economy engaged in industry have been modest, with rich countries experiencing a slight deindustrialization and poor countries experiencing slight gains in industry employment However, the data also show that the decrease in the percentage of the economy engaged in agriculture is very positively correlated with income. Conversely, again, although that the absolute gains in the service industry may be modest in poor countries, in relative terms, the poorest countries were the ones that have experienced the most dramatic shifts to a service-based economy.

```{r gdpcap-time-plot, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 8, fig.height = 3, out.width = "100%", fig.align='center', fig.cap = "1991-2019 GDP per Capita by Income Group"}
GDP_yearly_average %>% 
  ggplot(aes(x = year, y = gdp, colour = `Income Group`)) +
  geom_point() +
  labs(xlab = "Year", ylab = "Mean GDP (PPP) per Capita", title = "GDP per Capita by Income Group") +
  theme(plot.title = element_text(hjust=0.5))
```

Figure \@ref(fig:gdpcap-time-plot) shows the limitations of relying on static, 2020 classifications of country by income group. We can see from the data that rich countries have grown enormously while poor countries have remained trapped in low levels of growth. This begs the question of whether or not these poor economies have really not experienced growth, or whether there have just been a number of poor countries that have "graduated" to the status of a richer country. And, looking at the above charts, if this phenomenon exists, would the poor countries that emphasize services over agriculture be the ones that graduate to rich countries, thus giving us the massive differences across income groups that we see? These questions and many more motivate [Part II: Economic Structure and Economic Growth], where I, quite ambitiously, attempt to assess how resource allocation affects economic growth over the short-to-medium term.

### Model 1: Recurrent Neural Network

Given that the relationship between all of these inputs and economic growth is highly nonlinear (if we could so easily rely on coefficients to determine `X` feature's contribution to economic growth, development economists would have solved the growth trap of least developed countries already!) I decided to build a neural network to model the relationship between resource distribution and economic outcomes. And, since the data is truly panel data - it is absolutely impossible to consider any response value without understanding the identity of the country that it pertains to as well as the `n` values that came before it temporally - I chose to create and train a Recurrent Neural Network. 

Considering the strength of the relationship between economic structure and GDP per capita found in [Exploratory Data Analysis (Data Exploration)], I decided to construct a very simple model that only used four input features to determine the output (real GDP per capita.) These four input features were: `employment in agriculture (% of total)`, `employment in industry (% of total)`, `employment in services (% of total)` and the year of the observation.

After wrangling the data into three dimensions, splicing it into sections of length 5 and then wrangling those into another three-dimensional array, I built a simple three layer recurrent neural network. It used ReLU activation functions at each step and, since the output was continuous, the output node was singular. The input shape was (5, 4) because it took 4 features (agriculture, industry, manufacturing, services and year) over 5 distinct years.

```{r model-pt1-summary, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 4, out.width = "100%", fig.align='center', fig.cap = "Summary of RNN Model from Part 1"}
summary(model_part_1)
```

```{r, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 4, out.width = "100%", fig.align='center', fig.cap = "Training History of RNN Model from Part 1"}
plot_model_history(model_part_1$history$history)
```
Using the handy imported `plot_model_history` function from class, we can see that the model learns quite a bit over the 100 epochs it's run. The predictions of the RNN model actually get quite good. Unfortunately, since the problem is not a classification one, there is no data to plot for the `accuracy` portion of this graph. 

Exactly how good are the predictions of the model?

```{r model1-abs-acc, cache=TRUE, eval=TRUE, echo=FALSE}
model_part_1 = load_model_hdf5("model_part_1.h5")
tribble(~abs_loss, as.numeric(tensorflow::evaluate(model_part_1, X_test_RNN_continuous_GDP, y_test_RNN_continuous_GDP, verbose = FALSE)[2])) %>%
  kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
                        col.names = c("Absolute Test Error"),
        caption = "Accuracy of RNN Model on Test Data") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
  
```

The model, with very minimal tuning, Table \@ref(tab:model1-abs-acc) shows that this model can predict real GDP per capita within around 5500 USD. That's not perfect, but according to 2021 figures, that's substantially *less* than the difference between the US and Canada or the difference between Australia and the UK. It's, perhaps as expected, very directionally accurate with just the above four features.

Again, this model does not have very much tuning. I decided to save time to have the vast majority of tuning be for my models for [Part II: Economic Structure and Economic Growth], which is more involved and more relevant to what I wanted to study with this project. 

## Part II: Economic Structure and Economic Growth

This section is fundamentally different from the above [Part I: Economic Structure and Economic Development Level] in two ways. First, this section uses vastly many more features to try to generate a more complex and predictive model (28 in total, as opposed to Part I's 4.) Second, while the Part I looked at the relationship between the distribution of employment in an economy and its contemporaneous GDP per capita, this section aims to understand how the allocation of resources (interpreted more broadly than just labor this time) affect economic *growth* in the short-to-medium term.

Again, for this section, I manually created four different response variables. Each represented the percentage change in GDP from year 0 to year n, with the time periods being 1 year, 3 years, 5 years and 10 years.  

### Model 1: Linear Methods (Linear Models for Panel Data)

Since ordinary least squares is no good on panel data, I used the `plm` panel data estimators function from the `plm` package to create these linear models. Through the incorporation of time- and entity-invariant fixed effects, such a model can provide much more reliable coefficient estimates for panel data. 

To tune these models, I gave each formulation an extremely high degree of personalization; I tried unique combinations of all the following features:

First, I tried the different time periods to define "economic growth" as the response variable. Second, I tried models with one to eight lags in them. Each lag is an interaction term of a value from a previous year which takes a coefficient (that may or may not be significant). Lastly, I tried three models of lags: models that contained lags of GDP per capita from previous years, lags that contained annual growth rates from previous years, and models that contained both kinds of lags.  

```{r, echo=FALSE, cache=TRUE}
tibble(personalization_feature = c("Response Variable", "Lag Number", "Lag Type"),
       types = c("1, 3, 5, 10 year growth period", 
                 "1 to 8 lags",
                 "GDP per capita, annual growth or both")) %>%
  kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
                        col.names = c("Personalizaation Feature",
                                      "Types"),
        caption = "Features of Different Linear Model Specifications Tried") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
  
  
```

```{r plm-tuning-results, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 9, fig.height = 4, out.width = "100%", fig.align='center', fig.cap = "Comparison of the Results of Different Parameters for the Linear Model"}
param_grid_combined %>% 
  pivot_longer(cols = c(scaled_1yr, scaled_3yr, scaled_5yr, scaled_10yr),
               names_to = "growth_period",
               values_to = "scaled_test_mse") %>%
  ggplot(aes(x = lags, y = scaled_test_mse, 
             shape = growth_period, colour = lag_type)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(xlab = "Number of Lags", ylab="Scaled Test MSE",
       title = "Comparison of Parameters for Linear Model") +
  theme(plot.title = element_text(hjust=0.5))
```
Because of the high variance in test MSE, the number of significant features and significant lags across all different linear model specifications, I bootstrapped the calculations. As you can see in Figure \@ref(fig:plm-tuning-results), there still is a very high degree of variance in the results. However, there are a few conclusions that I drew from playing around the different tuning parameters. The first is that past annual growth rates are a much better predictor of future growth than past GDP per capita, as evidenced by the red lines in Figure \@ref(fig:plm-tuning-results) hanging much lower than the green ones. The second clear conclusion is that the lags are a much better predictor of future growth than most of the other features in the model. The lags are almost always significant, while most features almost always are not. Lastly, since in absolute test MSE as the growth period extended only increased, I standardized the test MSE values with respect to the growth period that they have as a response variable. Once standardized, it interestingly enough doesn't appear that a longer growth period tends to lead to worse predictive power.

```{r plm-var-imp-tibble, echo=FALSE, cache=TRUE}
wdi_series %>% filter(`Series Code` %in% indicators) %>% 
  select(`Series Code`, `Indicator Name`) %>% 
  inner_join(enframe(FINAL_times_plm_coef_sig), by = c("Series Code" = "name")) %>%
  arrange(desc(value)) %>% 
  kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
                        col.names = c("Indicator Code",
                          "Indicator Name",
                                      "Times in Model Feature Signficiant"),
        caption = "Variable Importance in Linear Model") %>%
  column_spec(2, width="3in") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
  
  
```

Table \@ref(tab:plm-var-imp-tibble) attempts to assess feature importance by counting the number of times that each feature was significant in a regression in the bootstrap sequence in [Model 1: Linear Methods (Linear Models for Panel Data)]. From the findings presented in Table \@ref(tab:plm-var-imp-tibble), perhaps one of the most interesting conclusion is that the variables that were used to build the RNN model above, which aimed to predict economic development level, were quite literally the least important features in the data set for predicting *economic growth*. However, the models show that there is a set of other variables which are quite important when it comes to predicting GDP growth. Gross capital formation makes a lot of sense, since more capital accumulation will inherently allow for more output. This was exactly the logic behind Stalinism: extract surplus from the peasants in order to increase the capital per worker ratio. `Natural resource rents` is an interesting important variable, since traditional economics would argue that resource rents prevent a country from innovating a dynamic economy away from those resource endowments (The "resource curse"). Or, since this chart only describes variable importance, it's very possible that the conclusion to be made from here is that the majority of resource rich countries (e.g. Saudi Arabia, United Arab Emirates, Norway, Canada) manage their mineral wealth wisely.


```{r sig-lags-features, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 8, fig.height = 4, out.width = "100%", fig.align='center', fig.cap = "Number of Significant Lags and Features as Function of Lags"}
# sig features 
param_grid_combined %>% 
  mutate(avg_sig_figs = (num_sig_features_1yr + 
                           num_sig_features_3yr +
                           num_sig_features_5yr + 
                           num_sig_features_10yr)/4,
         avg_sig_lags = (num_sig_lags_1yr + 
                           num_sig_lags_3yr +
                           num_sig_lags_5yr + 
                           num_sig_lags_10yr)/4) %>%
  group_by(lags) %>%
  summarise(avg_sig_figs = mean(avg_sig_figs),
            avg_sig_lags = mean(avg_sig_lags)) %>%
  ggplot(aes(x = lags)) +
  geom_point(aes(y = avg_sig_figs, colour = "red")) + 
  geom_smooth(aes(y = avg_sig_figs, colour = "red"), se = FALSE) +
  geom_point(aes(y = avg_sig_lags, colour = "blue")) + 
  geom_smooth(aes(y = avg_sig_lags, colour = "blue"),se = FALSE) +
  labs(xlab = "Number of Lags", ylab = "Average Number Significant",
       title = "Average Number of Significant Lags and Features") +
  scale_color_manual(labels = 
                       c("Lags", 
                         "Features"), 
                     values = c("blue", "red")) +
  theme(plot.title = element_text(hjust=0.5))
  
  
  

```

Figure \@ref(tab:sig-lags-features) shows that, as we add more lags to the model, they are almost always significant. Given this phenomenon, though, it's quite interesting that the number of significant features only decreases incredibly slightly as the the number of lags increases. The number of significant features stays pretty consistent around 10, even when plots like \@ref(fig:plm_var_importance_graph) show that those approximately 10 features must be very different in different bootstrapped models.


### Model 2: Recurrent Neural Network

For the recurrent neural network portion of this assignment, I decided to construct neural networks that tune along two different parameters. The first parameter tuned for is model complexity, defined as the number of layers in the neural network and the number of nodes in each layer. For this parameter, since there are infinite many values it could take, I chose three different models, one with one layer, one with two layers and one with three. The second parameter I tuned for is the temporal length of each "strip" of data. In other words, when the RNN model is given a sequence of values to determine the predicted outcome, how many years of previous data are represented in this strip? For this component, I chose the values 3, 4 and 5 years of previous data. I tried every combination of these 2 different parameters for a total of 9 different RNN models created. 

```{r rnn-param-comparison, echo=FALSE, eval=TRUE, cache=TRUE}
read.table("rnn-model-comparison") %>% 
  kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
                        col.names = c("Model Complexity",
                          "3 Previous Values",
                                      "4 Previous Values",
                          "5 Previous Values"),
        caption = "Comparison of RNN Model test MSE against Specifications") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
  
  
```

After training each model, each model was then run on a 20% share of the data held out as validation data. Interestingly enough, although the simplest model which looks at 5 years of previous data had the worst test MSE, Table \@ref(tab:rnn-param-comparison) shows the best parameter selection was that of the two-layer model that looks at 5 years of previous data. The reason that the one-layer 5-previous-year model is so bad compared to the other models is that, with more years of data, the model needs more parameters in order to be able to better fit the underlying trends. 

```{r rnn-feature-scramble, eval=TRUE, echo=FALSE, cache=TRUE}
read.table("rnn-feature-scramble") %>% left_join((wdi_series %>% filter(`Series Code` %in% indicators) %>% 
  select(`Series Code`, `Indicator Name`)),
  by = c("feature_name" = "Series Code")) %>% select(-feature_number) %>%
    kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
                        col.names = c("Deviance",
                          "Feature Code",
                                      "Feature Name"),
        caption = "Most Importance Features according to RNN Model") %>%
  column_spec(2, width="3in") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
  
```

To determine feature importance, I scrambled the values of each f and observed how much it reduced the accuracy of the predictions. Perhaps as I should have expected, the most important feature by far according to Table \@ref(tab:rnn-feature-scramble) is GDP per capita. This is interesting, because in the linear model, using GDP per capita as past lags was not nearly as powerful as using annual GDP per capita growth rates as the lags. 

Even more interestingly, the second most important variable in this model is `Agriculture, forestry, and fishing, value added per worker`. This is also quite interesting and unexpected, since we've observed that richer economies transition out of agriculture and into services. However, this might make sense, since in rich economies that have transitioned out of agriculture, only the most productive workers will remain in the industry. This can easily be seen by how farmers in a country like the US have heavy machinery, fertilizers and pesticides which will increase their yields to something that smallholder farmers in low income countries with just wooden tools could only dream of.

However, the results from this model are quite alarming, since the majority of the coefficients are very close to zero. In fact, there are even a few features whose values, when scrambled, slightly *improve* the test MSE. This is ridiculous, and I conclude this is a function of there just not being enough data to create a neural network model for this problems in this domain.


# Conclusions

## Method Comparison

When comparing the recurrent neural network model with the linear model, it's clear that, in terms of absolute reduction in test MSE, the neural network appears to do much better. However, models are only as good as they are useful, and assessing feature importance through scrambling for the neural network revealed that the model may as well have been guessing. It didn't offer very much insight into which variables were the truly important ones driving economic growth. Most features had importances near zero in the model, and some for some features, scrambling resulted in an even *worse* prediction than on the unscrambled data. 

In contrast, by counting how often a coefficient on the the linear model was significant, we were able to learn a lot more about the influence of each of the constituent variables in the model. The linear model better satisfied many of our preconceived notions that sparsity does *not* hold, and that each of these decorrelated features should have some sort of unique contribution, however large, to the economic growth and development of an economy. This, however, comes with the caveat that we assume that the relationships between these variables and the response is anything but linear. Moreover, we assume that all of these features have unique and complex interactions, which would imply that a neural network would potentially be a better model if we were given the data.

In conclusion, given that data in this problem was so limited, I believe that linear models were a much better fit for this problem than neural networks ever could be. 

## Takeaways

I think that perhaps one of the most powerful components of this project was [Part I: Economic Structure and Economic Development Level]. The simplicity of the model I used may be frowned upon by economists who say that a lack of variables would mean that my findings would be worthless because of hidden variable bias. This is without a doubt a problem with such simplicity, but on the other hand, such simplicity allows for such great model interpretability that you may learn a lot more from analyzing a lot less. 

In terms of interpreting the coefficients from the linear model, the most shocking conclusion is that perhaps there was a grain of truth in the horrors committed by dictator Joseph Stalin. It shows that the most important variable by far is capital accumulation per worker. This would lend credence to the theory that a government, in order to grow it's economy, should intervene to reduce consumption and thus increase the capital stock, which can then be utilized to create more capital stock. 

Another fascinating observation is that the three features I used to construct a relatively accurate recurrent neural network in [Part I: Economic Structure and Economic Development Level], the share of employment in agriculture, industry and services, though great at predicting current GDP per capita, are declared by the linear model in [Model 1: Linear Methods (Linear Models for Panel Data)] to be the least useful features in the data set.

## Limitations

Perhaps the greatest limitation of this study was just a lack of data. Before value imputation, the data was really spare and prevented me from choosing a lot of the indicators that I wanted. Moreover, I can only reasonably extrapolate a certain amount from trends in the data, so the amount of value imputation that I could do was limited. There are only so many countries in the world and years on each country for which we have data. Furthermore, through value imputation, I'm very concerned that, since I'm using each income group as the group from which to calculate the averages, I am systematically making poor countries look more poor and rich countries look more rich. This would be an insidious kind of bias that really hinders the model.

This lack of data truly hindered the accuracy of any model on this very complex phenomenon, whether it was linear, a neural network or some other form of model. This lack of data is something that economists have been struggling with forever, and is potentially why most economists are too shy to create a "theory of everything" model that tries to predict how much an economy will grow given a set of inputs. 
Secondarily, in the development of the neural network, the choice of the parameters is very much more of an art than a science. It's very possible that if I added/substracted a few layers, added in a few more nodes, increased dropout et cetera, I would have developed a model that is substantially better than the one I currently have. However, since trying an infinite number of parameters is computationally infeasible, we will never know the answer to these hypothetical questions. 

## Follow-ups

To follow up on this, I think that I would try some tree methods on this problem, since they were not tried. Tree methods don't need nearly as much data as neural networks, so they wouldn't have as much of a problem with the scarcity of data. Moreover, tree methods can allow for much more complex interactions among the features of a data set than linear methods, so there may be a lot of merit to their value in this domain.

I think to solve the main problem, more data has to be gathered. There are only so many different countries in the world, but fortunately not every country is of the same size. Many countries that numerous regions inside of them that are as large as and economically (in)dependent as any other country. For that reason, I think it would be a phenomenal next step to repeat this methodology, but using panel data disaggregated to the level of the subnational region.

Lastly, I want to say that economists have very traditionally relied on their econometric methods, attempting to assess the significance of variables and determine causality, and have not necessarily dipped into the very valuable vault of methods that data science has to offer. With more data, suddenly the simplicity of a linear difference-in-differences methodology seems very risky, because it's hard to expect such a relationship to be simply linear. 
