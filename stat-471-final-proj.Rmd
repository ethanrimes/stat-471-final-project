---
title: "stat-471-final-proj"
author: "Ethan"
date: "12/2/2021"
output:
  pdf_document: default
---

# Executive Summary

##Problem

In 1933, after Josef Stalin replaced the then-deceased Vladimir Lenin as premier of the Soviet Union, he looked around at the much wealthier nations to his West and noticed one key commonality – Great Britain, France and Germany all had well-developed industrial sectors. Stalin made the reallocation of resources from agriculture to industry perhaps the defining feature of his three decades of rule, even when it came at the cost of the lives of 35+ million of his own citizens.

This study sought to measure the impact of a “Big Push” – a massive reallocation of capital and labor from less productive (lower value-added) segments of the economy to higher value-added 
Data

## Data

The majority of the data came from the World Bank Development Indicators. They are one of the world’s most comprehensive datasets on economic development by country, with nearly 1500 indicators across 90 topics gathered annually at the country level since 1960. However, since there was a sparsity of data on some indicators, I chose indicators which contained a minimum threshold of data and imputed values for the remaining NA values.

Additionally, since I believed that data relating to armed conflict would be very relevant, I obtained this data from the Uppsala Conflict Data Program.

## Analysis

The first part of my project consists of an exploratory data analysis that attempts to understand the relationship between the allocation of resources across the major sectors of an economy and the level of economic development. I do this through the use of many summary statistics and graphs that explore the distribution of the labor across agriculture, industry and GDP per capita, as well as create a Recurrent Neural Network (RNN) to model this relationship.

The main focus of this assignment attempts to look at the relationship between the allocation of these resources and economic growth. I first pull 27 different covariates from the World Bank Development Indicators and a 28th focused on the level of armed conflict in different countries. First, I conducted a panel regression (through the plm package) of real GDP per capita growth (over various time periods) on the covariates identified and the different numbers of lags. Following this, I create a Recurrent Neural Network that predicts GDP per capita growth using the 28 features and a time length of 5 years for the samples. 

## Conclusions

# Introduction

## Background Information

The original motivation for this topic was my senior thesis in economics. Attempts to assess how the “China shock” heterogeneously impacts Brazil through trade. The enormity of Chinese demand for commodities, in particular soy, petroleum and iron, coupled with the prohibitively competitive efficiency of Chinese manufacturing, has led to an enormous shift in the focus of the Brazilian economy from industry to agriculture and mining over the past 20 years. This is not problematic in itself, but one of the main concerns of this “deindustrialization” of the country is that it is a huge barrier to growth. I wanted to use some of the models and methods I had learned in class to see if this deindustrialization will have economic consequences for a country in the long run. 

Everything I learned about Stalinism in ECON 271: Foundations of a Market Economy just further stimulated my interest. In that class, we researched and debated quite a bit about whether or not Stalinism and the 30+ million deaths associated with it was necessary to create the relatively modern industrial superpower capable of defeating Nazi Germany in World War Two. 

In addition, a huge motivation for the initial exploratory data analysis of this assignment was based on the very broad generalization of agricultural economies (e.g. Chad, Kenya, Ghana, Afghanistan) as poor economies, industrial economies (e.g. China, Thailand, Malaysia, Mexico) as middle income economies and service economies (e.g. the US, Japan, the UK, Israel) as wealthy economies. This logic follows from the fact that the amount of value added per labor hour increases in that order, so therefore the wage rate of economies should increase in that order.

## Analysis Goals
To put it succinctly, my principal analysis goals were to understand the relationship between the allocation of resources in an economy and its economic growth in the medium term. As a secondary area of interest, I wanted to understand how precisely the allocation of resources across various sectors of an economy (agriculture, industry and services) was able to determine it’s level of economic development. 

For the initial data exploration, in which I wanted to understand the strength of the relationship between the allocation of resources across different economic sectors and the country’s economic development, I used the allocation of labor across the three broad sectors of an economy (agriculture, industry and services) as the main features to predict the response. The data on this allocation of labor was surprisingly abundant. The response that I aimed to predict was GDP per capita, measured in current 2021 US dollars at purchasing power parity (PPP).

In the main focus of the assignment, I chose a selection of 28 different features from the World Bank Development Index that intended to either assess the allocation of resources within an economy or control for omitted variable bias. The outcome that I tried to predict was the percentage difference in an economy over the past several years. For the size/duration of this time frame, I tried a period of 1, 3, 5 and 10 years. 

Success is evaluated in various aspects. The first metric is test MSE, i.e. how well is the model able to predict outcomes on data that it has not been trained on. This is a good approximation of how well the model was able to fit the underlying data generating process. The numerical, objective nature of this metric also makes it great to compare different models. The second criterion is how well the models can differentiate between the different importances of different features. If the end goal is to understand what allocation of resources within an economy will help it grow the most in the medium term, it’s crucial to understand what levers are available to pull, which direction to pull them, and which levers are the most powerful.

## Significance

As countries across the world, particularly emerging market economies, strive to achieve sustainable growth to improve the quality of life of their citizens over the long run, studies like these are crucial. If a model can have a very low test MSE on the empirical data (meaning that it represents very well the underlying data generating process) and it is able to distinguish well between the features of which it is a part (indicating what are the important levers a “social planner” should pull), such a model would have enormous practical applications for governments across the world. Economic development leads to better economic lives for the majority of citizens of a country and aggregate gains in welfare can be huge.


```{r setup, cache = TRUE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, lazy.cache = FALSE)
library(tidyverse)
library(cowplot)
library(rnn)
library(caret)
library(keras)
library(plm)
library(kableExtra)
source("/Users/ethan/Documents/R/stat-471-fall-2021/stat-471-fall-2021/functions/deep_learning_helpers.R")
```

```{r imports, cache = TRUE, include=FALSE}
# a-z = 26 + aa-az = 26 + ba-bn = 14, sum = 66
# country name, country code, indicator name, indicator code, 1960-2020, empty final col
wdi_raw <- 
  read_csv("/Users/ethan/Documents/R/stat-471-final-project/WDI_csv/WDIData.csv",
           col_names = TRUE)
# for country code, (country) table name, region, income group, latest industrial data (year), latest trade data (year)
wdi_country <- 
  read_csv("/Users/ethan/Documents/R/stat-471-final-project/WDI_csv/WDICountry.csv",
           col_names = TRUE)
# for series code, topic, indicator name, short definition, long definition, periodicity, aggregation method
wdi_series <- 
  read_csv("/Users/ethan/Documents/R/stat-471-final-project/WDI_csv/WDISeries.csv",
           col_names = TRUE)
# conflict 
conflict_df <- 
  read_csv("/Users/ethan/Documents/R/stat-471-final-project/conflict.csv",
           col_names = TRUE)
```


Part 1: Economic Structure and Income per Capita (EDA)

```{r plots-p1-p4, cache = TRUE, include=FALSE}
# this part is almost an EDA

# let's create a table showing average shares by income group over time
holy_trinity <- wdi_raw %>% 
  filter(`Indicator Code` %in% c("SL.SRV.EMPL.ZS", 
                                 "SL.IND.EMPL.ZS", 
                                 "SL.AGR.EMPL.ZS"))

summary_trinity_time_income_group <- wdi_country %>% 
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`) %>%
  inner_join(holy_trinity, by = c("Short Name" = "Country Name")) %>%
  group_by(`Short Name`, `Indicator Code`, `Income Group`) %>%
  summarise(`90s` = mean(`1991`,`1992`,`1993`,`1994`,`1995`,`1996`,
                         `1997`,`1998`,`1999`, na.rm=TRUE),
            `00s` = mean(`2000`,`2001`,`2002`,`2003`,`2004`,`2005`,`2006`,`2007`,
                         `2008`,`2009`, na.rm=TRUE),
            `10s` = mean(`2010`,`2011`,`2012`,`2013`,`2014`,`2015`,`2016`,
                         `2017`,`2018`,`2019`, na.rm=TRUE))

summary_trinity_time_income_group_90s <- 
  summary_trinity_time_income_group %>%
  select(`Indicator Code`, `Income Group`, `90s`) %>%
  pivot_wider(names_from = `Indicator Code`, values_from = `90s`) %>%
  transmute(`Income Group` = as.factor(`Income Group`), 
            `Agriculture` = SL.AGR.EMPL.ZS, # change variable names
            `Industry` = SL.IND.EMPL.ZS,
            `Services` = SL.SRV.EMPL.ZS) %>%
  pivot_longer(cols = c(`Agriculture`,
                        `Industry`,
                        `Services`),
               values_to = "value",
               names_to = "Sector") %>%
  group_by(`Income Group`, Sector) %>% 
  summarise(value = mean(value, na.rm =TRUE)) %>% # calculate average share per sector
  ungroup() %>%
  mutate(income_group_order = case_when( # order the income groups
    `Income Group` == "Low income" ~ 1,
    `Income Group` == "Lower middle income" ~ 2,
    `Income Group` == "Upper middle income" ~ 3,
    `Income Group` == "High income" ~ 4
  ))
p1 <- summary_trinity_time_income_group_90s %>%
  ggplot(aes(x = Sector, y = value, fill = 
               fct_reorder(.f = summary_trinity_time_income_group_90s$`Income Group`, .x = summary_trinity_time_income_group_90s$income_group_order))) +
  geom_bar(position="dodge", stat="identity") +
  labs(x = "Sector", y = "Share of Employment",
       title = "1991-1999 Average Employment by Sector by Income Group") +
  guides(fill=guide_legend(title="Income Group")) +
  theme(text = element_text(size=9),
        legend.key.size = unit(0.5, 'cm'),
        plot.title = element_text(hjust=0.5)) 
  
summary_trinity_time_income_group_00s <- 
  summary_trinity_time_income_group %>%
  select(`Indicator Code`, `Income Group`, `00s`) %>%
  pivot_wider(names_from = `Indicator Code`, values_from = `00s`) %>%
  transmute(`Income Group` = as.factor(`Income Group`), 
            `Agriculture` = SL.AGR.EMPL.ZS, # change variable names
            `Industry` = SL.IND.EMPL.ZS,
            `Services` = SL.SRV.EMPL.ZS) %>%
  pivot_longer(cols = c(`Agriculture`,
                        `Industry`,
                        `Services`),
               values_to = "value",
               names_to = "Sector") %>%
  group_by(`Income Group`, Sector) %>% 
  summarise(value = mean(value, na.rm =TRUE)) %>% # calculate average share per sector
  ungroup() %>%
  mutate(income_group_order = case_when( # order the income groups
    `Income Group` == "Low income" ~ 1,
    `Income Group` == "Lower middle income" ~ 2,
    `Income Group` == "Upper middle income" ~ 3,
    `Income Group` == "High income" ~ 4
  ))
p2 <- summary_trinity_time_income_group_00s %>%
  ggplot(aes(x = Sector, y = value, fill = 
               fct_reorder(.f = summary_trinity_time_income_group_00s$`Income Group`, .x = summary_trinity_time_income_group_00s$income_group_order))) +
  geom_bar(position="dodge", stat="identity") +
  labs(x = "Sector", y = "Share of Employment",
       title = "2000-2009 Average Employment by Sector by Income Group") +
  guides(fill=guide_legend(title="Income Group")) +
  theme(text = element_text(size=9),
        legend.key.size = unit(0.5, 'cm'),
        plot.title = element_text(hjust=0.5)) 

summary_trinity_time_income_group_10s <- 
  summary_trinity_time_income_group %>%
  select(`Indicator Code`, `Income Group`, `10s`) %>%
  pivot_wider(names_from = `Indicator Code`, values_from = `10s`) %>%
  transmute(`Income Group` = as.factor(`Income Group`), 
            `Agriculture` = SL.AGR.EMPL.ZS, # change variable names
            `Industry` = SL.IND.EMPL.ZS,
            `Services` = SL.SRV.EMPL.ZS) %>%
  pivot_longer(cols = c(`Agriculture`,
                        `Industry`,
                        `Services`),
               values_to = "value",
               names_to = "Sector") %>%
  group_by(`Income Group`, Sector) %>% 
  summarise(value = mean(value, na.rm =TRUE)) %>% # calculate average share per sector
  ungroup() %>%
  mutate(income_group_order = case_when( # order the income groups
    `Income Group` == "Low income" ~ 1,
    `Income Group` == "Lower middle income" ~ 2,
    `Income Group` == "Upper middle income" ~ 3,
    `Income Group` == "High income" ~ 4
  ))
p3 <- summary_trinity_time_income_group_10s %>%
  ggplot(aes(x = Sector, y = value, fill = 
               fct_reorder(.f = summary_trinity_time_income_group_10s$`Income Group`, .x = summary_trinity_time_income_group_10s$income_group_order))) +
  geom_bar(position="dodge", stat="identity") +
  labs(x = "Sector", y = "Share of Employment",
       title = "2010-2019 Average Employment by Sector by Income Group") +
  guides(fill=guide_legend(title="Income Group")) +
  theme(text = element_text(size=9),
        legend.key.size = unit(0.5, 'cm'),
        plot.title = element_text(hjust=0.5)) 
```
```{r share-change-over-time, cache = TRUE, include=FALSE}
# lets see how the shares of each sector evolve over time
shares_change_over_time <- wdi_country %>% 
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`) %>%
  inner_join(holy_trinity, by = c("Short Name" = "Country Name")) %>% 
  pivot_longer(cols=`1991`:`2019`, names_to = "year", values_to = "share") %>%
  select(`Short Name`, `Income Group`, `Indicator Code`, year, share) %>%
  drop_na() %>%
  group_by(`Indicator Code`, year, `Income Group`) %>%
  summarise(share = mean(share)) %>%
  transmute(year = year, share = share,`Income Group`=`Income Group`,
            indicator = as.factor(case_when(
              `Indicator Code` == "SL.AGR.EMPL.ZS" ~ "Agriculture",
              `Indicator Code` == "SL.IND.EMPL.ZS" ~ "Industry",
              `Indicator Code` == "SL.SRV.EMPL.ZS" ~ "Services"
            )))
shares_change_over_time %>%
  ggplot(aes(x = year, y = share, colour=indicator)) +
  geom_point() +
  facet_wrap(~`Income Group`) +
  scale_x_discrete(breaks = seq(1991, 2019, by = 5))
  
```
```{r shares-change-tibble, cache = TRUE, include=FALSE}
high_inc_agr_1991 =  shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 1991,
         indicator == "Agriculture") %>% pull(share)
high_inc_agr_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 2019,
         indicator == "Agriculture") %>% pull(share)
high_inc_ind_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 1991,
         indicator == "Industry") %>% pull(share)
high_inc_ind_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 2019,
         indicator == "Industry") %>% pull(share)
high_inc_srv_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 1991,
         indicator == "Services") %>% pull(share)
high_inc_srv_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "High income",
         year == 2019,
         indicator == "Services") %>% pull(share)

highmid_inc_agr_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 1991,
         indicator == "Agriculture") %>% pull(share)
highmid_inc_agr_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 2019,
         indicator == "Agriculture") %>% pull(share)
highmid_inc_ind_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 1991,
         indicator == "Industry") %>% pull(share)
highmid_inc_ind_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 2019,
         indicator == "Industry") %>% pull(share)
highmid_inc_srv_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 1991,
         indicator == "Services") %>% pull(share)
highmid_inc_srv_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Upper middle income",
         year == 2019,
         indicator == "Services") %>% pull(share)

lowmid_inc_agr_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 1991,
         indicator == "Agriculture") %>% pull(share)
lowmid_inc_agr_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 2019,
         indicator == "Agriculture") %>% pull(share)
lowmid_inc_ind_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 1991,
         indicator == "Industry") %>% pull(share)
lowmid_inc_ind_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 2019,
         indicator == "Industry") %>% pull(share)
lowmid_inc_srv_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 1991,
         indicator == "Services") %>% pull(share)
lowmid_inc_srv_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Lower middle income",
         year == 2019,
         indicator == "Services") %>% pull(share)

low_inc_agr_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 1991,
         indicator == "Agriculture") %>% pull(share)
low_inc_agr_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 2019,
         indicator == "Agriculture") %>% pull(share)
low_inc_ind_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 1991,
         indicator == "Industry") %>% pull(share)
low_inc_ind_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 2019,
         indicator == "Industry") %>% pull(share)
low_inc_srv_1991 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 1991,
         indicator == "Services") %>% pull(share)
low_inc_srv_2019 = shares_change_over_time %>% 
  filter(`Income Group` == "Low income",
         year == 2019,
         indicator == "Services") %>% pull(share)

# percent change over time across sectors and income groups, precipitous decline in ag, increase in services
pct_change_groups_time <- tibble(
  income_group = c("High income", "Upper middle income", "Lower middle income",
                   "Low income"),
  change_agriculture = c((high_inc_agr_2019 - high_inc_agr_1991)/high_inc_agr_1991,
                         (highmid_inc_agr_2019 - highmid_inc_agr_1991)/highmid_inc_agr_1991,
                         (lowmid_inc_agr_2019 - lowmid_inc_agr_1991)/lowmid_inc_agr_1991,
                         (low_inc_agr_2019 - low_inc_agr_1991)/low_inc_agr_1991),
  change_industry = c((high_inc_ind_2019 - high_inc_ind_1991)/high_inc_ind_1991,
                         (highmid_inc_ind_2019 - highmid_inc_ind_1991)/highmid_inc_ind_1991,
                         (lowmid_inc_ind_2019 - lowmid_inc_ind_1991)/lowmid_inc_ind_1991,
                         (low_inc_ind_2019 - low_inc_ind_1991)/low_inc_ind_1991),
  change_services = c((high_inc_srv_2019 - high_inc_srv_1991)/high_inc_srv_1991,
                         (highmid_inc_srv_2019 - highmid_inc_srv_1991)/highmid_inc_srv_1991,
                         (lowmid_inc_srv_2019 - lowmid_inc_srv_1991)/lowmid_inc_srv_1991,
                         (low_inc_srv_2019 - low_inc_srv_1991)/low_inc_srv_1991)
)
```
```{r calculate-boxplot, cache = TRUE, include=FALSE}
# variance in each group and each time period
summary_trinity_time_income_group <- summary_trinity_time_income_group %>%
  mutate(income_group_order = case_when( # order the income groups
    `Income Group` == "Low income" ~ 1,
    `Income Group` == "Lower middle income" ~ 2,
    `Income Group` == "Upper middle income" ~ 3,
    `Income Group` == "High income" ~ 4
  ),
  Indicator = as.factor(case_when(
              `Indicator Code` == "SL.AGR.EMPL.ZS" ~ "Agriculture",
              `Indicator Code` == "SL.IND.EMPL.ZS" ~ "Industry",
              `Indicator Code` == "SL.SRV.EMPL.ZS" ~ "Services"
            )))

ninteties_boxplot <- summary_trinity_time_income_group %>%
  ggplot(aes(y = `90s`, x = fct_reorder(.f = summary_trinity_time_income_group$`Income Group`, .x = summary_trinity_time_income_group$income_group_order), colour=Indicator)) +
  geom_boxplot() +
  labs(x = "Income Group", y = "Share of Employment",
       title = "90s Distribution of Emp. by Sector by Income Group") +
  guides(fill=guide_legend(title="Sector")) +
  theme_bw()
  
millenium_boxplot <- summary_trinity_time_income_group %>%
  ggplot(aes(y = `00s`, x = fct_reorder(.f = summary_trinity_time_income_group$`Income Group`, .x = summary_trinity_time_income_group$income_group_order), colour=Indicator)) +
  geom_boxplot() +
  labs(x = "Income Group", y = "Share of Employment",
       title = "00s Distribution of Emp. by Sector by Income Group") +
  guides(fill=guide_legend(title="Sector")) +
  theme_bw()
  
tens_boxplot <- summary_trinity_time_income_group %>%
  ggplot(aes(y = `10s`, x = fct_reorder(.f = summary_trinity_time_income_group$`Income Group`, .x = summary_trinity_time_income_group$income_group_order), colour=Indicator)) +
  geom_boxplot() +
  labs(x = "Income Group", y = "Share of Employment",
       title = "10s Distribution of Emp. by Sector by Income Group") +
  guides(fill=guide_legend(title="Sector")) +
  theme(text = element_text(size=9),
        legend.key.size = unit(0.5, 'cm'),
        plot.title = element_text(hjust=0.5)) 
```

```{r calc-observations, cache = TRUE, include=FALSE}
# calculate number of observations to work with 
trinity_filtered <- wdi_country %>% 
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`) %>%
  inner_join(holy_trinity, by = c("Short Name" = "Country Name")) %>%
  select(-`Indicator Name`, -`Country Code`, -`2020`, -`...66`, -(`1960`:`1990`)) %>%
  drop_na()

trinity_filtered
# 516 rows, which means 172 countries with data from 1991-2019 (29 years of data)
```

```{r, cache = TRUE, include=FALSE}
# get data of employment shares and GDP
gdp_pcap_ppp_countries <- wdi_raw %>% 
  filter(`Indicator Code` %in% c("NY.GDP.PCAP.PP.CD")) %>%
  inner_join(
    (wdi_country %>% 
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`)),
    by = c("Country Name" = "Short Name")
  ) %>%
  select(`1991`:`2019`, `Country Name`, `Income Group`, `Indicator Code`) %>%
  rename(`Short Name` = `Country Name`)

within_model_data <- rbind(trinity_filtered, gdp_pcap_ppp_countries) %>%
  arrange(`Short Name`)

# remove the countries without economy share data
within_model_data <- 
  within_model_data %>% group_by(`Short Name`) %>% 
  summarise(count = n()) %>%
  ungroup() %>% 
  filter(count > 3) %>%
  select(-count) %>%
  inner_join(within_model_data,
             by = c("Short Name" = "Short Name"))

# value imputation
# replace all NA GDP values with the average GDP for the average in that income group for that year
average_GDP_stats <- within_model_data %>% group_by(`Income Group`) %>%
  filter(`Indicator Code` == "NY.GDP.PCAP.PP.CD") %>%
  summarise(`1991` = mean(`1991`, na.rm=TRUE),`1992` = mean(`1992`, na.rm=TRUE),
            `1993` = mean(`1993`, na.rm=TRUE),`1994` = mean(`1994`, na.rm=TRUE),
            `1995` = mean(`1995`, na.rm=TRUE),`1996` = mean(`1996`, na.rm=TRUE),
            `1997` = mean(`1997`, na.rm=TRUE),`1998` = mean(`1998`, na.rm=TRUE),
            `1999` = mean(`1999`, na.rm=TRUE),`2000` = mean(`2000`, na.rm=TRUE),
            `2001` = mean(`2001`, na.rm=TRUE),`2002` = mean(`2002`, na.rm=TRUE),
            `2003` = mean(`2003`, na.rm=TRUE),`2004` = mean(`2004`, na.rm=TRUE),
            `2005` = mean(`2005`, na.rm=TRUE),`2006` = mean(`2006`, na.rm=TRUE),
            `2007` = mean(`2007`, na.rm=TRUE),`2008` = mean(`2008`, na.rm=TRUE),
            `2009` = mean(`2009`, na.rm=TRUE),`2010` = mean(`2010`, na.rm=TRUE),
            `2011` = mean(`2011`, na.rm=TRUE),`2012` = mean(`2012`, na.rm=TRUE),
            `2013` = mean(`2013`, na.rm=TRUE),`2014` = mean(`2014`, na.rm=TRUE),
            `2015` = mean(`2015`, na.rm=TRUE),`2016` = mean(`2016`, na.rm=TRUE),
            `2017` = mean(`2017`, na.rm=TRUE),`2018` = mean(`2018`, na.rm=TRUE),
            `2019` = mean(`2019`, na.rm=TRUE))

# impute the null values with averages  
within_model_data_imputed <- within_model_data
for(row in 4:nrow(within_model_data_imputed)) {
  income_group <- as.character(within_model_data_imputed[row,2])
  for (col in 4:32) {
    if (is.na(within_model_data_imputed[row,col])) {
      within_model_data_imputed[row,col] <-
        (average_GDP_stats %>% filter(`Income Group` == income_group))[1,col-2]
    }
    col = col + 1
  }
  row = row + 4
}
```
```{r, include=FALSE}
GDP_yearly_average <- average_GDP_stats %>% 
  pivot_longer(-`Income Group`, names_to = "year", values_to = "gdp")
GDP_yearly_average$year <- as.numeric(GDP_yearly_average$year)

GDP_yearly_average %>% 
  ggplot(aes(x = year, y = gdp, colour = `Income Group`)) +
  geom_point()

# You see that high income countries have grown a lot, but not low income countries
# Potentially because low income countries that grow graduate to higher rankings, so you just have fewer countries
# In these categories
```

RNN: Continuous Regression on GDP per Capita

```{r, cache = TRUE, include=FALSE}
num_countries <- dim(within_model_data_imputed %>% group_by(`Short Name`) %>%
  summarise(n()))[1]
num_years <- 2019-1991 + 1
num_features <- 4 # add the year

# set a random seed for reproducability
set.seed(123)


within_model_data_imputed <- 
  within_model_data_imputed %>% arrange(`Short Name`, `Indicator Code`)
# GDP, AGR, IND, SRV

# put data into 3d array
data_3d <- array(dim = c(num_countries, num_years, num_features+1)) # add year
for (row in 1:nrow(within_model_data_imputed)) {
  #print(row)
  feature <- row %% num_features 
  if (feature == 0) {
    feature <- num_features 
  }
  #print(feature)
  country = trunc((row+(num_features-1))/num_features)
  for (col_year in 1:num_years) {
    data_3d[country, col_year, feature] <- 
      within_model_data_imputed[row, col_year+3] %>%
      as.numeric()
  }
}

# add year as a variable
for (year in 1:num_years) {
  data_3d[,year,5] <- year+1990
}

```


```{r, cache = TRUE, include=FALSE}
# cut into sections
# set some parameters for our model
max_len <- 4 # the number of previous examples we'll look at
stride <- 1 # striding between segments when selecting time series data
segs_per_country <- (num_years - max_len)/stride

# get a list of start indexes for our (overlapping) chunks
data_segmented <- array(dim = c(num_countries*segs_per_country, 
                                max_len+1, num_features+1)) # add year
for(row in 1:(dim(data_3d)[1])) {
  for(starting_col in 1:segs_per_country) {
    seg_row <- ((row-1)*segs_per_country) + starting_col
    data_segmented[seg_row,,] <- data_3d[row,starting_col:(starting_col+max_len),]
  }
}
```

```{r, cache = TRUE, include=FALSE}
# split into test and train
X_RNN_continuous_GDP <- data_segmented[,,-1]
y_RNN_continuous_GDP <- data_segmented[,,1]

set.seed(43)
train_indices <- sample(1:(dim(data_segmented)[1]), dim(data_segmented)[1]*.8)

# training data
X_train_RNN_continuous_GDP <- X_RNN_continuous_GDP[train_indices,,]
y_train_RNN_continuous_GDP <- y_RNN_continuous_GDP[train_indices,]

# testing data
X_test_RNN_continuous_GDP <- X_RNN_continuous_GDP[-train_indices,,]
y_test_RNN_continuous_GDP <- y_RNN_continuous_GDP[-train_indices,]


```

```{r, include=FALSE}
batch_size <- 64 # number of sequences to look at at one time during training
total_epochs <- 100 # how many times we'll look @ the whole dataset while training our model

model_part_1 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_train_RNN_continuous_GDP)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 64, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32, name="RNN_2",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32, name="RNN_3",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

model_part_1 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit
```

```{r, cache = TRUE, include=FALSE}
# Actually train our model! This step will take a while
model_part_1 %>% fit(
    x = X_train_RNN_continuous_GDP, # sequence we're using for prediction 
    y = y_train_RNN_continuous_GDP, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.33) # how much data to hold out for testing as we go along

```



Part 2: Economic Structure and Income per Capita Growth

```{r, cache = TRUE, include=FALSE}
# feature codes
# outcome
vars1 <- c("NY.GDP.PCAP.CD") #"GDP per capita (current US$)"
# agriculture
vars2 <- c("NV.AGR.TOTL.ZS", # Agriculture, forestry, and fishing, value added (% of GDP)
           "SL.AGR.EMPL.ZS", # Employment in agriculture (% of total employment) 
           "NV.AGR.EMPL.KD", # Agriculture, forestry, and fishing, value added per worker (constant 2015 US$)
           "NY.GDP.TOTL.RT.ZS") # Total natural resources rents (% of GDP)
# industry
vars3 <- c("NV.IND.TOTL.ZS", # Industry (including construction), value added (% of GDP)
           "NV.MNF.TECH.ZS.UN", # Medium and high-tech manufacturing value added (% manufacturing value added)
           "SL.IND.EMPL.ZS") # Employment in industry (% of total employment) 
# services
vars4 <- c("NV.SRV.TOTL.ZS", # Services, value added (% of GDP)
           "SL.SRV.EMPL.ZS") # Employment in services (% of total employment) 
# trade
vars5 <- c("NE.EXP.GNFS.ZS", # Exports of goods and services (% of GDP)
           "BG.GSR.NFSV.GD.ZS", # Employment in agriculture (% of total employment) 
           "NE.IMP.GNFS.ZS") # Imports of goods and services (% of GDP)
# macroeconomics
vars6 <- c("FP.CPI.TOTL.ZG", # Inflation, consumer prices (annual %)
           "FS.AST.PRVT.GD.ZS", # Domestic credit to private sector (% of GDP)
           "DT.ODA.ODAT.PC.ZS", # Net ODA received per capita (current US$)
           "NE.GDI.TOTL.ZS", # Gross capital formation (% of GDP)
           "SI.POV.UMIC", # Poverty headcount ratio at $5.50 a day (2011 PPP) (% of population)
           "SI.POV.GINI") # Gini index (World Bank estimate)
# human capital
vars7 <- c("SP.URB.TOTL.IN.ZS", # Urban population (% of total population)
           "SP.POP.1564.TO.ZS", # Population ages 15-64 (% of total population)
           "SL.TLF.CACT.ZS", # Labor force participation rate, total (% of total population ages 15+)
           "SL.TLF.CACT.FM.ZS", # Ratio of female to male labor force participation rate (%)
           "SE.PRM.ENRR", # School enrollment, primary (% gross)
           "SE.PRM.ENRL.FE.ZS", # Primary education, pupils (% female)
           "SE.TER.ENRR", # School enrollment, tertiary (% gross)
           "SE.SEC.ENRL.GC.FE.ZS" # Secondary education, general pupils (% female)
           # ,"SE.SEC.ENRL.VO" # Secondary education, vocational pupils
           ) 
indicators <- c(vars1,vars2,vars3,vars4,vars5,vars6,vars7)
# conflict
conflict_deaths <- conflict_df %>% group_by(country, year) %>%
  summarise(yearly_conflict_deaths = (sum(best))^.5) %>% # raised to .5 to reflect diminishing marginal negative effect as the conflict scales
  ungroup() %>% 
  pivot_wider(names_from = year, values_from = yearly_conflict_deaths) %>%
  ungroup()
conflict_deaths[is.na(conflict_deaths)] = 0
conflict_deaths <- conflict_deaths %>% mutate(
  `Indicator Code` = "Conflict",
  `Indicator Name` = "Armed conflict deaths"
)
conflict_deaths <- conflict_deaths %>% mutate(`Country Name` = 
  case_when(country == "Bosnia-Herzegovina" ~ "Bosnia and Herzegovina",
  country == "Cambodia (Kampuchea)" ~ "Cambodia",
  country == "DR Congo (Zaire)" ~ "Congo, Dem. Rep.",
  country == "Gambia" ~ "Gambia, The",
  country == "Ivory Coast" ~ "Côte d'Ivoire",
  country == "Kingdom of eSwatini (Swaziland)" ~ "Eswatini",
  country == "Kyrgyzstan" ~ "Kyrgyz Republic",
  country == "Laos" ~ "Lao PDR",
  country == "Macedonia, FYR" ~ "North Macedonia",
  country == "Madagascar (Malagasy)" ~ "Madagascar",
  country == "Myanmar (Burma)" ~ "Myanmar",
  country == "Russia (Soviet Union)" ~ "Russia",
  country == "Serbia (Yugoslavia)" ~ "Serbia",
  country == "Syria" ~ "Syrian Arab Republic",
  country == "United States of America" ~ "United States",
  country == "Venezuela" ~ "Venezuela",
  country == "Yemen (North Yemen)" ~ "Yemen",
  country == "Zimbabwe (Rhodesia)" ~ "Zimbabwe",
  TRUE ~ country))
conflict_deaths <- conflict_deaths %>% left_join(
  (wdi_country %>%
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`)),
  by = c("Country Name" = "Short Name")
)
venezuela <- tribble( # manually create income group for Venezuela
  ~country, ~group,
  "Venezuela", "Upper middle income"
)
conflict_deaths <- 
  conflict_deaths %>% 
  left_join(venezuela,
            by = c("Country Name" = "country", "Income Group" = "group"))

# extract final data set to use
indicators_complete <- 
  wdi_raw %>% filter(`Indicator Code` %in% indicators) %>%
  inner_join( 
    (wdi_country %>%
  filter(!(is.na(`Currency Unit`))) %>% # remove regional groupings
  select(`Short Name`, `Income Group`)),
  by = c("Country Name" = "Short Name")
  )

indicators_complete <- indicators_complete %>% 
  select(-(`1960`:`1983`), -`...66`, -`Indicator Name`)
conflict_deaths <- conflict_deaths %>%
  mutate(
    `1984` = NA,
    `1985` = NA,
    `1986` = NA,
    `1987` = NA,
    `1988` = NA
  ) %>% select(-country)
indicators_complete <- indicators_complete %>% 
  relocate(where(is.numeric), .after = where(is.character)) %>%
  select(-`Country Code`)
conflict_deaths <- conflict_deaths %>% 
  relocate(where(is.numeric), .after = where(is.character)) %>%
  select(-`Indicator Name`)
indicators_complete_joined <- rbind(indicators_complete, conflict_deaths) %>%
  filter(!(`Country Name` %in% c("Congo", "Côte d'Ivoire", "Dem. Rep. Congo", 
                                 "Egypt", "Iran", "Russia", "The Gambia", "Venezuela", "Yemen",
                                 "Gambia, The", "Congo, Dem. Rep.")))
# growth metric = difference between year 0 and year n
# other growth metric = avg growth in period 
zero_conflict <- indicators_complete_joined %>% group_by(`Country Name`, `Income Group`) %>%
  summarise(count = n()) %>% ungroup() %>% filter(count == 27) %>% select(-count)

# manually add empty conflict row
zero_conflict <- zero_conflict %>% mutate(`Indicator Code` = "Conflict")
zero_matrix <- as_tibble(matrix(0, nrow = dim(zero_conflict)[1], ncol = (dim(indicators_complete_joined)[2]-3)))
year_names <- colnames(indicators_complete_joined %>% select(`1984`:`2020`))
for (i in 1:(dim(indicators_complete_joined)[2]-3)) {
  names(zero_matrix)[i] <- year_names[i]
}
indicators_complete_joined <- rbind(
  cbind(
  zero_conflict %>% mutate(`Indicator Code` = "Conflict"),
  zero_matrix), 
indicators_complete_joined
)
```


```{r, cache = TRUE, include=FALSE}
# value imputation
indicators_imputed <- indicators_complete_joined
for (row in 1:(dim(indicators_complete_joined)[1])) {
  indicator = indicators_complete_joined[row,3] %>% as.character()
  income_group = indicators_complete_joined[row,2] %>% as.character()
  country_avg <- mean(indicators_complete_joined[row,(4:(dim(indicators_complete_joined)[2]))] %>%
                        unlist(use.names=FALSE),
           na.rm = TRUE)
  if (row %% 100 == 0) {
    print(row)
  }
  for (col in 1:(dim(indicators_complete_joined)[2]-3)) {
    val = indicators_complete_joined[row,(dim(indicators_complete_joined)[2] - col+1)] %>%
      as.numeric()
    if (is.na(val)) {
      year <- as.character(dim(indicators_complete_joined)[2] - col+1 +1980)
      year_avg <- mean(indicators_complete_joined %>% # year avg
        filter(
          `Income Group` == income_group,
          `Indicator Code` == indicator
        ) %>%
        pull(year), na.rm=TRUE)
      
    val <- mean(c(year_avg, country_avg), na.rm=TRUE)
    indicators_imputed[row,(dim(indicators_complete_joined)[2] - col+1)] <- val
    }
  }
}

# run twice to really get rid of all NaNs
for (row in 1:(dim(indicators_imputed)[1])) {
  indicator = indicators_imputed[row,3] %>% as.character()
  income_group = indicators_imputed[row,2] %>% as.character()
  country_avg <- mean(indicators_imputed[row,(4:(dim(indicators_imputed)[2]))] %>%
                        unlist(use.names=FALSE),
           na.rm = TRUE)
  if (row %% 100 == 0) {
    print(row)
  }
  for (col in 1:(dim(indicators_imputed)[2]-3)) {
    val = indicators_imputed[row,(dim(indicators_imputed)[2] - col+1)] %>%
      as.numeric()
    #print(val)
    if (is.na(val)) {
      year <- as.character(dim(indicators_imputed)[2] - col+1 +1980)
      year_avg <- mean(indicators_imputed %>% # year avg
        filter(
          `Income Group` == income_group,
          `Indicator Code` == indicator
        ) %>%
        pull(year), na.rm=TRUE)
      
    val <- mean(c(year_avg, country_avg), na.rm=TRUE)
    indicators_imputed[row,(dim(indicators_imputed)[2] - col+1)] <- val
    }
  }
}

indicators_imputed <- indicators_imputed %>% drop_na() # drop na country names

```
Variable selection with best subset regression


```{r, cache = TRUE, include=FALSE}
# Wrangle into correct format
indicators_plm <- indicators_imputed %>%
  pivot_longer(cols = (`1984`:`2020`), names_to = "Year", values_to = "Value") %>%
  pivot_wider(names_from="Indicator Code", values_from = "Value") %>%
  select(-`Income Group`)
indicators_plm$Year = as.numeric(indicators_plm$Year)
indicators_plm$`1yr_growth` <- NA
indicators_plm$`3yr_growth` <- NA
indicators_plm$`5yr_growth` <- NA
indicators_plm$`10yr_growth` <- NA

obs_country <- indicators_plm %>% group_by(`Country Name`) %>%
  summarize(count = n()) %>% head(1) %>% pull(count)
num_countries <- dim(indicators_plm %>% group_by(`Country Name`) %>%
  summarize(count = n()))[1]
gdp_vals <- indicators_plm %>% select(NY.GDP.PCAP.CD) %>% unlist()

country_as_id <- indicators_plm %>% group_by(`Country Name`) %>%
  summarise(country_id = n()) %>% ungroup()
country_as_id$country_id = c(1:(dim(country_as_id)[1]))
indicators_plm <- indicators_plm %>% 
  inner_join(country_as_id, by = c("Country Name" = "Country Name"))

# fill in for train
for (row in 1:(dim(indicators_plm)[1])) {
  num_years_history <- (row - 1) %% obs_country
  this_year <- gdp_vals[row]
  # fill 1 year
  if (num_years_history > 0) {
    yr0 <- gdp_vals[row - 1]
    indicators_plm$`1yr_growth`[row] = (this_year-yr0)/yr0
  }
  # fill 3 year
  if (num_years_history > 2) {
    yr0 <- gdp_vals[row - 3]
    indicators_plm$`3yr_growth`[row] = (this_year-yr0)/yr0
  }
  # fill 5 year
  if (num_years_history > 4) {
    yr0 <- gdp_vals[row - 5]
    indicators_plm$`5yr_growth`[row] = (this_year-yr0)/yr0
  }
  # fill 10 year
  if (num_years_history > 9) {
    yr0 <- gdp_vals[row - 10]
    indicators_plm$`10yr_growth`[row] = (this_year-yr0)/yr0
  }
}
```

```{r, cache = TRUE, include=FALSE}
# tune for number of lags and key variables
max_lags <- 8
num_bootstrap_iterations <- 12
gdp_growth_lags_grid = expand.grid(
  lags = c(1:max_lags),
  test_mse_1yr = 0,
  num_sig_features_1yr = 0,
  num_sig_lags_1yr = 0,
  test_mse_3yr = 0,
  num_sig_features_3yr = 0,
  num_sig_lags_3yr = 0,
  test_mse_5yr = 0,
  num_sig_features_5yr = 0,
  num_sig_lags_5yr = 0,
  test_mse_10yr = 0,
  num_sig_features_10yr = 0,
  num_sig_lags_10yr = 0
)
gdp_pcap_lags_grid <- gdp_growth_lags_grid
gdp_both_lags_grid <- gdp_growth_lags_grid

growth_bootstrap_grid = expand.grid(
  iteration = c(1:num_bootstrap_iterations),
  test_mse_1yr = 0,
  num_sig_features_1yr = 0,
  num_sig_lags_1yr = 0,
  test_mse_3yr = 0,
  num_sig_features_3yr = 0,
  num_sig_lags_3yr = 0,
  test_mse_5yr = 0,
  num_sig_features_5yr = 0,
  num_sig_lags_5yr = 0,
  test_mse_10yr = 0,
  num_sig_features_10yr = 0,
  num_sig_lags_10yr = 0
)
pcap_bootstrap_grid <- growth_bootstrap_grid
both_bootstrap_grid <- growth_bootstrap_grid

get_lags <- function(response, limit) {
  str <- ""
  if (limit > 0) {
    
    for (index in 1:limit) {
    to_add = paste(" +lag(", response, ",", as.character(index), ")")
    str <- paste(str, to_add)
  }
  }
  str
}
all_features <- "0 + NV.AGR.TOTL.ZS + NV.AGR.EMPL.KD + FS.AST.PRVT.GD.ZS + SL.AGR.EMPL.ZS  +  SL.IND.EMPL.ZS  +  SL.SRV.EMPL.ZS +   NE.EXP.GNFS.ZS   +  SI.POV.GINI   +    NE.GDI.TOTL.ZS  +    NE.IMP.GNFS.ZS  +    NV.IND.TOTL.ZS  +    FP.CPI.TOTL.ZG +   SL.TLF.CACT.ZS  +     NV.MNF.TECH.ZS.UN  +     DT.ODA.ODAT.PC.ZS  +     SP.POP.1564.TO.ZS  +   SI.POV.UMIC  +     SE.PRM.ENRL.FE.ZS  +     SL.TLF.CACT.FM.ZS  +     SE.PRM.ENRR  +     SE.TER.ENRR   +   SE.SEC.ENRL.GC.FE.ZS +    NV.SRV.TOTL.ZS  +     NY.GDP.TOTL.RT.ZS  +   BG.GSR.NFSV.GD.ZS  +     SP.URB.TOTL.IN.ZS  +      Conflict"

get_1yr_response <- function(test_data, lags) {
  no_na <- test_data %>% 
    select(-`3yr_growth`, -`5yr_growth`, -`10yr_growth`) %>%
  drop_na()
  min_year <- no_na %>% arrange(Year) %>% head(1) %>% pull(Year)
  test_data %>% filter(Year > (min_year + lags)) %>% pull(`1yr_growth`)
}
get_3yr_response <- function(test_data,lags) {
  no_na <- test_data %>% 
    select(-`1yr_growth`, -`5yr_growth`, -`10yr_growth`)  %>%
    drop_na()
  min_year <- no_na %>% arrange(Year) %>% head(1) %>% pull(Year)
  test_data %>% filter(Year > (min_year + lags)) %>% pull(`3yr_growth`)
}
get_5yr_response <- function(test_data, lags) {
  no_na <- test_data %>% 
    select(-`3yr_growth`, -`1yr_growth`, -`10yr_growth`) %>%
    drop_na()
  min_year <- no_na %>% arrange(Year) %>% head(1) %>% pull(Year)
  test_data %>% filter(Year > (min_year + lags)) %>% pull(`5yr_growth`)
}
get_10yr_response <- function(test_data, lags) {
  no_na <- test_data %>% 
    select(-`3yr_growth`, -`5yr_growth`, -`1yr_growth`) %>%
    drop_na()
  min_year <- no_na %>% arrange(Year) %>% head(1) %>% pull(Year)
  test_data %>% filter(Year > (min_year + lags)) %>% pull(`10yr_growth`)
}

for (i in 0:max_lags) {
  print("lag number")
  print(i)
  # calls
  call_1yr_growth <- paste("X1yr_growth ~ ", all_features, get_lags("X1yr_growth", i))
  call_3yr_growth <- paste("X3yr_growth ~ ", all_features, get_lags("X3yr_growth", i))
  call_5yr_growth <- paste("X5yr_growth ~ ", all_features, get_lags("X5yr_growth", i))
  call_10yr_growth <- paste("X10yr_growth ~ ", all_features, get_lags("X10yr_growth", i))
  
  call_1yr_pcap <- paste("X1yr_growth ~ ", all_features, get_lags("NY.GDP.PCAP.CD", i))
  call_3yr_pcap <- paste("X3yr_growth ~ ", all_features, get_lags("NY.GDP.PCAP.CD", i))
  call_5yr_pcap <- paste("X5yr_growth ~ ", all_features, get_lags("NY.GDP.PCAP.CD", i))
  call_10yr_pcap <- paste("X10yr_growth ~ ", all_features, get_lags("NY.GDP.PCAP.CD", i))
  
  call_1yr_both <- paste("X1yr_growth ~ ", all_features, get_lags("X1yr_growth", i), get_lags("NY.GDP.PCAP.CD", i))
  call_3yr_both <- paste("X3yr_growth ~ ", all_features, get_lags("X3yr_growth", i), get_lags("NY.GDP.PCAP.CD", i))
  call_5yr_both <- paste("X5yr_growth ~ ", all_features, get_lags("X5yr_growth", i), get_lags("NY.GDP.PCAP.CD", i))
  call_10yr_both <- paste("X10yr_growth ~ ", all_features, get_lags("X10yr_growth", i), get_lags("NY.GDP.PCAP.CD", i))
  
  for (iteration in 1:num_bootstrap_iterations) {
    
    # split into train and test
    set.seed(iteration*3)
    train_countries <- sample(1:num_countries, .8*num_countries)
      indicators_plm_train <- indicators_plm %>%
    filter(country_id %in% train_countries)
      indicators_plm_test <- indicators_plm %>%
    filter(!(country_id %in% train_countries))
      
      pseries_train <- pseriesfy(pdata.frame(as.data.frame(indicators_plm_train), 
                         index = c("country_id", "Year")))
      pseries_test <- pseriesfy(pdata.frame(as.data.frame(indicators_plm_test), 
                         index = c("country_id", "Year")))


    # models
  model_1yr_growth <- plm(call_1yr_growth, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_1yr_growth_coefs <- summary(model_1yr_growth)$coefficients
  model_1yr_growth_tibble <- as_tibble(model_1yr_growth_coefs) %>%
    mutate(variable = rownames(model_1yr_growth_coefs))
  model_3yr_growth <- plm(call_3yr_growth, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_3yr_growth_coefs <- summary(model_3yr_growth)$coefficients
  model_3yr_growth_tibble <- as_tibble(model_3yr_growth_coefs) %>%
    mutate(variable = rownames(model_3yr_growth_coefs))
  model_5yr_growth <- plm(call_5yr_growth, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_5yr_growth_coefs <- summary(model_5yr_growth)$coefficients
  model_5yr_growth_tibble <- as_tibble(model_5yr_growth_coefs) %>%
    mutate(variable = rownames(model_5yr_growth_coefs))
  model_10yr_growth <- plm(call_10yr_growth, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_10yr_growth_coefs <- summary(model_10yr_growth)$coefficients
  model_10yr_growth_tibble <- as_tibble(model_10yr_growth_coefs) %>%
    mutate(variable = rownames(model_10yr_growth_coefs))
  
  model_1yr_pcap <- plm(call_1yr_pcap, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_1yr_pcap_coefs <- summary(model_1yr_pcap)$coefficients
  model_1yr_pcap_tibble <- as_tibble(model_1yr_pcap_coefs) %>%
    mutate(variable = rownames(model_1yr_pcap_coefs))
  model_3yr_pcap <- plm(call_3yr_pcap, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_3yr_pcap_coefs <- summary(model_3yr_pcap)$coefficients
  model_3yr_pcap_tibble <- as_tibble(model_3yr_pcap_coefs) %>%
    mutate(variable = rownames(model_3yr_pcap_coefs))
  model_5yr_pcap <- plm(call_5yr_pcap, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_5yr_pcap_coefs <- summary(model_5yr_pcap)$coefficients
  model_5yr_pcap_tibble <- as_tibble(model_5yr_pcap_coefs) %>%
    mutate(variable = rownames(model_5yr_pcap_coefs))
  model_10yr_pcap <- plm(call_10yr_pcap, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_10yr_pcap_coefs <- summary(model_10yr_pcap)$coefficients
  model_10yr_pcap_tibble <- as_tibble(model_10yr_pcap_coefs) %>%
    mutate(variable = rownames(model_10yr_pcap_coefs))
  
  model_1yr_both <- plm(call_1yr_both, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_1yr_both_coefs <- summary(model_1yr_both)$coefficients
  model_1yr_both_tibble <- as_tibble(model_1yr_both_coefs) %>%
    mutate(variable = rownames(model_1yr_both_coefs))
  model_3yr_both <- plm(call_3yr_both, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_3yr_both_coefs <- summary(model_3yr_both)$coefficients
  model_3yr_both_tibble <- as_tibble(model_3yr_both_coefs) %>%
    mutate(variable = rownames(model_3yr_both_coefs))
  model_5yr_both <- plm(call_5yr_both, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_5yr_both_coefs <- summary(model_5yr_both)$coefficients
  model_5yr_both_tibble <- as_tibble(model_5yr_both_coefs) %>%
    mutate(variable = rownames(model_5yr_both_coefs))
  model_10yr_both <- plm(call_10yr_both, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within")
  model_10yr_both_coefs <- summary(model_10yr_both)$coefficients
  model_10yr_both_tibble <- as_tibble(model_10yr_both_coefs) %>%
    mutate(variable = rownames(model_10yr_both_coefs))
  
  # results
  # 1yr response
  resp_1yr <- get_1yr_response(indicators_plm_test, i) 
  growth_bootstrap_grid$test_mse_1yr[iteration] =
    mean((predict(model_1yr_growth,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_1yr)^2, na.rm=TRUE)
  pcap_bootstrap_grid$test_mse_1yr[iteration] =
    mean((predict(model_1yr_pcap,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_1yr)^2, na.rm=TRUE)
  both_bootstrap_grid$test_mse_1yr[iteration] = 
    mean((predict(model_1yr_both,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_1yr)^2, na.rm=TRUE)
  
  # 3yr response
  resp_3yr <- get_3yr_response(indicators_plm_test, i)
  growth_bootstrap_grid$test_mse_3yr[iteration] =
    mean((predict(model_3yr_growth,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_3yr)^2, na.rm=TRUE)
  pcap_bootstrap_grid$test_mse_3yr[iteration] =
    mean((predict(model_3yr_pcap,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_3yr)^2, na.rm=TRUE)
  both_bootstrap_grid$test_mse_3yr[iteration] = 
    mean((predict(model_3yr_both,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_3yr)^2, na.rm=TRUE)
  # 5 yr response
  resp_5yr <- get_5yr_response(indicators_plm_test, i)
  growth_bootstrap_grid$test_mse_5yr[iteration] =
    mean((predict(model_5yr_growth,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_5yr)^2, na.rm=TRUE)
  pcap_bootstrap_grid$test_mse_5yr[iteration] =
    mean((predict(model_5yr_pcap,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_5yr)^2, na.rm=TRUE)
  both_bootstrap_grid$test_mse_5yr[iteration] = 
    mean((predict(model_5yr_both,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_5yr)^2, na.rm=TRUE)
  # 10 yr response
  resp_10yr <- get_10yr_response(indicators_plm_test, i)
  growth_bootstrap_grid$test_mse_10yr[iteration] =
    mean((predict(model_10yr_growth,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_10yr)^2, na.rm=TRUE)
  pcap_bootstrap_grid$test_mse_10yr[iteration] =
    mean((predict(model_10yr_pcap,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_10yr)^2, na.rm=TRUE)
  both_bootstrap_grid$test_mse_10yr[iteration] = 
    mean((predict(model_10yr_both,
                  newdata = pseries_test,
                  index = c("country_id", "Year"), 
                  na.action= "na.omit",
                  model="within") - resp_10yr)^2, na.rm=TRUE)
  
  # extract number significant features
  growth_bootstrap_grid$num_sig_features_1yr[iteration] =
    model_1yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_features_3yr[iteration] =
    model_3yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_features_5yr[iteration] =
    model_5yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_features_10yr[iteration] =
    model_10yr_growth_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  
  pcap_bootstrap_grid$num_sig_features_1yr[iteration] =
    model_1yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_features_3yr[iteration] =
    model_3yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_features_5yr[iteration] =
    model_5yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_features_10yr[iteration] =
    model_10yr_pcap_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  
  both_bootstrap_grid$num_sig_features_1yr[iteration] =
    model_1yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  both_bootstrap_grid$num_sig_features_3yr[iteration] =
    model_3yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  both_bootstrap_grid$num_sig_features_5yr[iteration] =
    model_5yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  both_bootstrap_grid$num_sig_features_10yr[iteration] =
    model_10yr_both_tibble %>% filter(!grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  
  # extract number significant lags
  growth_bootstrap_grid$num_sig_lags_1yr[iteration] =
    model_1yr_growth_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_lags_3yr[iteration] =
    model_3yr_growth_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_lags_5yr[iteration] =
    model_5yr_growth_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  growth_bootstrap_grid$num_sig_lags_10yr[iteration] =
    model_10yr_growth_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  
  pcap_bootstrap_grid$num_sig_lags_1yr[iteration] =
    model_1yr_pcap_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_lags_3yr[iteration] =
    model_3yr_pcap_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_lags_5yr[iteration] =
    model_5yr_pcap_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  pcap_bootstrap_grid$num_sig_lags_10yr[iteration] =
    model_10yr_pcap_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  
  both_bootstrap_grid$num_sig_lags_1yr[iteration] =
    model_1yr_both_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  both_bootstrap_grid$num_sig_lags_3yr[iteration] =
    model_3yr_both_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  both_bootstrap_grid$num_sig_lags_5yr[iteration] =
    model_5yr_both_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
 both_bootstrap_grid$num_sig_lags_10yr[iteration] =
    model_10yr_both_tibble %>% filter(grepl("lag", variable)) %>%
    filter(`Pr(>|t|)` < 0.5 | `Pr(>|t|)` > 0.95) %>% summarise(n = n()) %>% pull(n)
  }
  
  gdp_growth_lags_grid$test_mse_1yr[i] = mean(growth_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_features_1yr[i] = mean(growth_bootstrap_grid$num_sig_features_1yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_lags_1yr[i] = mean(growth_bootstrap_grid$num_sig_lags_1yr, na.rm=TRUE)
  gdp_growth_lags_grid$test_mse_3yr[i] = mean(growth_bootstrap_grid$test_mse_3yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_features_3yr[i] = mean(growth_bootstrap_grid$num_sig_features_3yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_lags_3yr[i] = mean(growth_bootstrap_grid$num_sig_lags_3yr, na.rm=TRUE)
  gdp_growth_lags_grid$test_mse_5yr[i] = mean(growth_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_features_5yr[i] = mean(growth_bootstrap_grid$num_sig_features_5yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_lags_5yr[i] = mean(growth_bootstrap_grid$num_sig_lags_5yr, na.rm=TRUE)
  gdp_growth_lags_grid$test_mse_10yr[i] = mean(growth_bootstrap_grid$test_mse_10yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_features_10yr[i] = mean(growth_bootstrap_grid$num_sig_features_10yr, na.rm=TRUE)
  gdp_growth_lags_grid$num_sig_lags_10yr[i] = mean(growth_bootstrap_grid$num_sig_lags_10yr, na.rm=TRUE)
  
  gdp_pcap_lags_grid$test_mse_1yr[i] = mean(pcap_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_features_1yr[i] = mean(pcap_bootstrap_grid$num_sig_features_1yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_lags_1yr[i] = mean(pcap_bootstrap_grid$num_sig_lags_1yr, na.rm=TRUE)
  gdp_pcap_lags_grid$test_mse_3yr[i] = mean(pcap_bootstrap_grid$test_mse_3yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_features_3yr[i] = mean(pcap_bootstrap_grid$num_sig_features_3yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_lags_3yr[i] = mean(pcap_bootstrap_grid$num_sig_lags_3yr, na.rm=TRUE)
  gdp_pcap_lags_grid$test_mse_5yr[i] = mean(pcap_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_features_5yr[i] = mean(pcap_bootstrap_grid$num_sig_features_5yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_lags_5yr[i] = mean(pcap_bootstrap_grid$num_sig_lags_5yr, na.rm=TRUE)
  gdp_pcap_lags_grid$test_mse_10yr[i] = mean(pcap_bootstrap_grid$test_mse_10yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_features_10yr[i] = mean(pcap_bootstrap_grid$num_sig_features_10yr, na.rm=TRUE)
  gdp_pcap_lags_grid$num_sig_lags_10yr[i] = mean(pcap_bootstrap_grid$num_sig_lags_10yr, na.rm=TRUE)

  
  gdp_both_lags_grid$test_mse_1yr[i] = mean(both_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_features_1yr[i] = mean(both_bootstrap_grid$num_sig_features_1yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_lags_1yr[i] = mean(both_bootstrap_grid$num_sig_lags_1yr, na.rm=TRUE)
  gdp_both_lags_grid$test_mse_3yr[i] = mean(both_bootstrap_grid$test_mse_3yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_features_3yr[i] = mean(both_bootstrap_grid$num_sig_features_3yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_lags_3yr[i] = mean(both_bootstrap_grid$num_sig_lags_3yr, na.rm=TRUE)
  gdp_both_lags_grid$test_mse_5yr[i] = mean(both_bootstrap_grid$test_mse_1yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_features_5yr[i] = mean(both_bootstrap_grid$num_sig_features_5yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_lags_5yr[i] = mean(both_bootstrap_grid$num_sig_lags_5yr, na.rm=TRUE)
  gdp_both_lags_grid$test_mse_10yr[i] = mean(both_bootstrap_grid$test_mse_10yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_features_10yr[i] = mean(both_bootstrap_grid$num_sig_features_10yr, na.rm=TRUE)
  gdp_both_lags_grid$num_sig_lags_10yr[i] = mean(both_bootstrap_grid$num_sig_lags_10yr, na.rm=TRUE)
}
```
```{r, cache = TRUE, include=FALSE}
gdp_growth_lags_grid <- as_tibble(gdp_growth_lags_grid) %>%
  mutate(lag_type = "growth")
gdp_pcap_lags_grid <- as_tibble(gdp_pcap_lags_grid) %>%
  mutate(lag_type = "gdp_pcap")
gdp_both_lags_grid <- as_tibble(gdp_both_lags_grid) %>%
  mutate(lag_type = "both")

param_grid_combined = rbind(
  gdp_growth_lags_grid,
  gdp_pcap_lags_grid,
  gdp_both_lags_grid
)

param_grid_combined$scaled_1yr <- scale(param_grid_combined$test_mse_1yr)
param_grid_combined$scaled_3yr <- scale(param_grid_combined$test_mse_3yr)
param_grid_combined$scaled_5yr <- scale(param_grid_combined$test_mse_5yr)
param_grid_combined$scaled_10yr <- scale(param_grid_combined$test_mse_10yr)

param_grid_combined %>% 
  pivot_longer(cols = c(scaled_1yr, scaled_3yr, scaled_5yr, scaled_10yr),
               names_to = "growth_period",
               values_to = "scaled_test_mse") %>%
  ggplot(aes(x = lags, y = scaled_test_mse, 
             shape = growth_period, colour = lag_type)) +
  geom_point() +
  geom_smooth(se = FALSE)

# we see that there is like no trend in the data at all lmao
# bootstrap it?
# take a look at the 
# growth seems to be a much better choice of lags than other kinds of lags 

```
```{r, cache = TRUE, include=FALSE}
# Occam's Razor Principle
best_call <- paste("X3yr_growth ~ ", all_features, get_lags("X3yr_growth", 4))
pseries <- pseriesfy(pdata.frame(as.data.frame(indicators_plm), 
                         index = c("country_id", "Year")))
summary(plm(best_call, data = pseries_train, 
                          index = c("country_id", "Year"),
                          na.action= "na.omit",
                          model="within"))

```

RNN model

```{r, cache = TRUE, include=FALSE}
# wrangle the data into 3 dimensions
to_3d_all_features <- indicators_plm %>% 
  select(-`1yr_growth`, -`5yr_growth`, -`10yr_growth`, -country_id) %>% # select 3 yr growth as response
  pivot_longer(cols=c(Conflict, NV.AGR.TOTL.ZS:`3yr_growth`), names_to = "Indicator Code", values_to = "vals") %>%
  filter(Year > 1986) %>% # filter out years with null response
  pivot_wider(names_from = Year, values_from = vals) %>%
  arrange(`Country Name`, `Indicator Code`) %>% drop_na()

to_3d_all_features %>% group_by(`Country Name`) %>%
  summarise(count = n())

num_countries <- dim(to_3d_all_features %>% group_by(`Country Name`) %>%
  summarise(n()))[1]
num_years <- 2020-1987 + 1
num_features <- dim(to_3d_all_features %>% group_by(`Indicator Code`) %>%
  summarise(n()))[1] # add the year

# set a random seed for reproducibility
set.seed(123)

# put data into 3d array
all_features_3d <- array(dim = c(num_countries, num_years, num_features+1)) # add year
for (row in 1:nrow(to_3d_all_features)) {
  #print(row)
  feature <- row %% num_features 
  if (feature == 0) {
    feature <- num_features 
  }
  #print(feature)
  country = trunc((row+(num_features-1))/num_features)
  for (col_year in 1:num_years) {
    all_features_3d[country, col_year, feature] <- 
      to_3d_all_features[row, col_year+2] %>%
      as.numeric()
  }
}

# add year as a variable
for (year in 1:num_years) {
  all_features_3d[,year,num_features+1] <- year+1990
}

```


```{r, cache = TRUE, include=FALSE}
# cut into sections
# set some parameters for our model
max_len <- 4 # the number of previous examples we'll look at
stride <- 1 # striding between segments when selecting time series data
segs_per_country <- (num_years - max_len)/stride

# get a list of start indexes for our (overlapping) chunks
segmented_full_data <- array(dim = c(num_countries*segs_per_country, 
                                max_len+1, num_features+1)) # add year
for(row in 1:(dim(all_features_3d)[1])) {
  for(starting_col in 1:segs_per_country) {
    seg_row <- ((row-1)*segs_per_country) + starting_col
    segmented_full_data[seg_row,,] <- all_features_3d[row,starting_col:(starting_col+max_len),]
  }
}
```

```{r, cache = TRUE, include=FALSE}
# split into test and train
X_RNN_full_data <- segmented_full_data[,,-1]
y_RNN_full_data <- segmented_full_data[,,1]

set.seed(43)
train_indices <- sample(1:(dim(segmented_full_data)[1]), dim(segmented_full_data)[1]*.8)

# training data
X_RNN_full_train <- X_RNN_full_data[train_indices,,]
y_RNN_full_train <- y_RNN_full_data[train_indices,]

# testing data
X_RNN_full_test <- X_RNN_full_data[-train_indices,,]
y_RNN_full_test <- y_RNN_full_data[-train_indices,]
```

```{r, cache = TRUE, include=FALSE}
batch_size <- 64 # number of sequences to look at at one time during training
total_epochs <- 100 # how many times we'll look @ the whole dataset while training our model


# model one
model_1 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

model_1 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit


# model two
model_2 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_2",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

model_2 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit

# model three
model_3 <- keras_model_sequential() %>%
  layer_dense(input_shape = dim(X_RNN_full_train)[2:3], units = max_len) %>%
  layer_simple_rnn(units = 128, name="RNN_1",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_2",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_simple_rnn(units = 64, name="RNN_3",
                   activation = "relu",
                   return_sequences = TRUE) %>%
  layer_dense(units = 1)

model_3 %>% compile(loss = "mean_squared_error",   # which loss to use
          optimizer = "RMSprop",     # how to optimize the loss
          metrics = c("mean_absolute_error"))             # how to evaluate the fit
```

```{r, cache = TRUE, include=FALSE}
# Actually train our model! This step will take a while
trained_model_1 <- model_1 %>% fit(
    x = X_RNN_full_train, # sequence we're using for prediction 
    y = y_RNN_full_train, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along

trained_model_2 <- model_2 %>% fit(
    x = X_RNN_full_train, # sequence we're using for prediction 
    y = y_RNN_full_train, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along

trained_model_3 <- model_3 %>% fit(
    x = X_RNN_full_train, # sequence we're using for prediction 
    y = y_RNN_full_train, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.25) # how much data to hold out for testing as we go along


```

```{r, cache = TRUE, include=FALSE}
# RNN model comparison
model_1_mean_abs_error <- 
  as.numeric(evaluate(model_1, X_RNN_full_test, y_RNN_full_test, verbose = FALSE)[2])
model_2_mean_abs_error <- 
  as.numeric(evaluate(model_2, X_RNN_full_test, y_RNN_full_test, verbose = FALSE)[2])
model_3_mean_abs_error <- 
  as.numeric(evaluate(model_3, X_RNN_full_test, y_RNN_full_test, verbose = FALSE)[2])

rnn_model_errs_tibble <- tribble(~model_1, ~model_2, ~model_3,
        model_1_mean_abs_error, model_2_mean_abs_error, model_3_mean_abs_error)
rnn_model_errs_tibble
```

```{r, cache = TRUE, include=FALSE}
# feature importance throgh scrambling
# pseudocode until I get internet
scrambled_feature_mse_vector <- rep(0, (dim(X_RNN_full_test)[3]-1))
for (feature in 1:(dim(X_RNN_full_test)[3]-1)) {
  scrambled_3d <- X_RNN_full_test
  for (t in 1:(dim(scrambled_3d)[2])) {
    scrambled_3d[,t,feature] <- sample(scrambled_3d[,t,feature],
                                       length(scrambled_3d[,t,feature]),
                                       replace = FALSE)
  }
  # print("scrambling feature")
  # print
  scrambled_feature_mse_vector[feature] <-
    as.numeric(evaluate(model_2, scrambled_3d, 
                        y_RNN_full_test, verbose = FALSE)[2])
}
model_2_err <- rnn_model_errs_tibble %>% pull(model_2)
feature_vector <- to_3d_all_features %>%
  slice(2:(dim(X_RNN_full_test)[3])) %>%
  pull(`Indicator Code`)
scrambled_feature_mse_vector
scrambled_feature_tibble <- tibble(feature_number = c(1:(dim(scrambled_3d)[3]-1)),
                                   deviance = scrambled_feature_mse_vector - model_2_err,
                                   feature_name = feature_vector) %>%
  arrange(desc(deviance))

scrambled_feature_tibble
```

# Data

## Data Sources

The vast majority of the data were collected from the World Bank Development Indicators. The Development Indicators can be accessed [here](https://databank.worldbank.org/source/world-development-indicators). The data contain over 1400 time series indicators for 217 different economies and over 40 supranational groupings, with several indicators going back over 50 years. Because the task of collecting such diffuse data is impossible, for one organization to manage, the World Bank sources a lot of the data from official sources such as national statistics organizations, United Nations agencies, academia and beyond. However, the World Bank Group does conduct several surveys and research projects of their own to add to the data set.

However, since I knew that armed conflict would have a huge influence on the macroeconomic growth of an observation country, I decided to join more data sourced from the [Uppsala Conflict Data Program (UCDP)](https://ucdp.uu.se/?id=1&id=1). The UCDP collects data on armed conflict since 1975. They define an "event" as any instance of fatal organized violence. And to ensure truly global representation in their data set, they utilize a keyword search through the Dow Jones Factiva Indicator, which annually returns around 50 thousand unique events which are then reviewed by human evaluators, resulting in around 10-12 new events coded each year. 

## Data Cleaning

To clean the data, the World Bank Development Indicators were first filtered to exclude non-country observations in the data. For the indicators, since many only have observations for a small select set of (usually wealthy and developed) countries, or only a short temporal range for the time series, or the many missing values in between where data was not collected for whatever reason, I first filtered for indicators that had adequate data for the model. This reduced the number of indicators from 1443 to 497. Then, from this list, I manually chose features which lacked high levels of multicollinearity, were expressed as a percentage rather than in absolute numbers (to avoid selection bias), and were very relevant to the project. Lastly, I removed countries without a certain availability of data and restricted the temporal range to 1984-2020, since the data became less abundant earlier in the set. In order to join the World Bank Development Indicators on the UCDP conflict data, some country names had to be manually adjusted. 

Following this selection of data, values were manually imputed to make sure there were no NaN values in the data set. This imputation was done by iterating through every value in the panel data set, and, if it was null, calculating a "yearly" average and an "observation" average and then averaging both. The yearly average consisted of the average value for that indicator in a given year among all countries in that income group ("High income", "Upper middle income", "Lower middle income", or "Low income"). The observation average was the mean value for a given indicator for a given country across all years in the data set. 

To wrangle the data into a format suitable for the time-series fixed effects regression in [Model 1: Linear Methods (Linear Models for Panel Data)], the data was first transformed into a `panel data dataframe` from the `plm` package, and then subsequently transformed into a `pseries` object from the package with the `Year` and `country_id` as indices. 

For the Recurrent Neural Network models, the data had to be wrangled into a three-dimensional format, where the first dimension represented observations, the second dimension represented time, and the third dimension represented the features of the model. This was done iteratively through for loops, parsing every value in the two-dimensional data and mapping it to its correct position in the three-dimensional array. Following this, each country time series was split into 5 year long strips with a stride of one (e.g. one strip for 1999-2003, one for 2000-2004, one for 2001-2005, etc.) and then aggregated into another 3-dimensional array.

## Data Description

```{r data-description-1, cache=TRUE, eval=TRUE, echo=FALSE}
indicators_imputed <- indicators_imputed %>% drop_na()
num_indicators <- dim(indicators_imputed %>% group_by(`Indicator Code`) %>%
  summarise())[1]
num_income_groups <- 4
num_years <- 2020-1984+1
num_countries <- dim(indicators_imputed %>% group_by(`Country Name`) %>%
  summarise())[1]
num_rows <- dim(indicators_imputed)[1]
num_cols <- dim(indicators_imputed)[2]
tibble(stat = c("Number of Indicators",
                "Number of Income Groups",
                "Number of Years Considered",
                "Number of Countries",
                "Number of Rows",
                "Number of Columns"),
       val = c(num_indicators, num_income_groups, num_years, 
               num_countries, num_rows, num_cols)) %>%
  kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
                        col.names = c("Summary Statistic",
                                      "Value"),
        caption = "Summary Statistics of Data with Value Imputation") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
```

Summary statistics of the data can be seen from Table \@ref(tab:data-description-1). There are a total of 5600 observations and 40 columns in the principal data set used, after value imputation. These observations can be divided into time series data (from 1984 to 2020) for 200 countries, grouped into 4 different income groups, across 28 different indicators. 

```{r feature-description, cache=TRUE, eval=TRUE, echo=FALSE}
initial_tibble <- 
  wdi_series %>% filter(`Series Code` %in% indicators) %>% 
  select(`Series Code`, `Indicator Name`) %>%
  add_row(`Series Code` = "Conflict", 
          `Indicator Name` = "Sqrt of conflict deaths given country and year") %>%
  add_column(Type = c("Continuous")) %>%
  kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
                        col.names = c("Indicator Code",
                                      "Indicator Name"),
        caption = "List of Indicators Used in Analysis") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
```

The 28 different features considered in the data set are listed in Table \@ref(tab:feature-description). However, these features should be considered only 27, since the variable `NY.GDP.PCAP.CD` or `GDP per capita (current US$)` was used to construct the response variables. In [Part II: Economic Structure and Economic Growth], the response variable was continuous, and was defined as a the percentage change in GDP of a given observation (country) over a certain period of time. This could be calculated as (GDP in year n - GDP in year 0)/GDP in year 0. The time periods of 1 year, 3 years, 5 years and 10 years were all tried on the data set. 

For [Part I: Economic Structure and Economic Development Level], the continuous feature `NY.GDP.PCAP.CD` was used directly as the response variable. 

## Data Allocation

To get good estimates for [Model 1: Linear Methods (Linear Models for Panel Data)], in this section, the values were bootstrapped and models were calculated from train/test splits multiple times. In each train/test split, 80% of the values were used for training. 20% of the values were used for testing, and test MSE, number of significant features and other metrics were calculated on this test data and aggregated. 

In [Model 2: Recurrent Neural Network], no bootstrapping was done, but 80% of the data was held in reserve as test data to assess model fit. During training, the neural network held 25% of the training data in reserve as validation data in order to tune the parameters. 

# Modeling

## Part I: Economic Structure and Economic Development Level

This first portion of data looked to see at the relationship between the structure of an economy at the most aggregate, generalized level and its economic development level, measured in real (purchasing power parity) GDP per capita. It consists of [Exploratory Data Analysis (Data Exploration)], which looks at summary statistics to understand this relationship and [Model 1: Recurrent Neural Network], which builds a RNN model to predict GDP (PPP) per capita.

### Exploratory Data Analysis (Data Exploration)

```{r plot-grid-p1-p3, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 6, fig.height = 7, out.width = "100%", fig.align='center', fig.cap = "Average Employment by Sector by Income Group Over Time"}
plot_grid(p1,p2,p3,ncol=1)
```

From Figure \@ref(fig:plot-grid-p1-p3), it is very visible that there is a clear correlation between the structure of an economy and its "income level", as classified by the World Bank. As a country gets richer, resources are very clearly taken from agriculture and reallocated to industry and services, with services being an even larger recipient than industry. Furthermore, we see that not only has this trend remains quite constant over time, but that the relative gaps between income groups in various industries also seems quite constant over the past three decades. 

```{r tens-boxplot, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 9, fig.height = 5, out.width = "100%", fig.align='center', fig.cap = "Distribution of Employment by Sector by Income Group 2010-2019"}
tens_boxplot
```

Figure \@ref(fig:tens-boxplot) further affirms the strength of the connection between economic mix and GDP per capita. Not only are the averages so disparate across income groups, but that distributions are also very tight. Outliers in this data set are very few, and even if you look at the outliers with the highest share of agriculture in the "High income group", they all have a lower share of the economy dedicated to agriculture than the median "Upper middle income" country. 


```{r share-over-time-change-plot, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 5, out.width = "100%", fig.align='center', fig.cap = "Sector Share Change by Income Group 1991-2019"}
shares_change_over_time %>%
  ggplot(aes(x = year, y = share, colour=indicator)) +
  geom_point() +
  facet_wrap(~`Income Group`) +
  scale_x_discrete(breaks = seq(1991, 2019, by = 5))
```

Figure \@ref(fig:share-over-time-change-plot) shows us the importance of controlling for time in any model. The relative share of the economy dedicated to services has exploded across all income groups. Likewise, the share dedicated to agriculture has fallen through time in all income groups, as well, though the decline was perhaps more precipitous in poorer countries than in richer countries. 

```{r pct-chg-groups-time-tibble, echo=FALSE, message=FALSE, warning=FALSE}
pct_change_groups_time %>% kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
                        col.names = c("Income Group",
                                      "% Change Agriculture",
                                      "% Change Industry",
                                      "% Change Services"),
        caption = "1991-2019 % Change in Share of Economy Dedicated to a Sector by Income Group") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
```

Table \@ref(tab:pct-chg-groups-time-tibble) again shows that the economies are not static. Subject to market forces, they have been evolving over time in an enormous way (think of how much time and resources it takes to train a smallholder farmer to be a teacher, bus driver, programmer, banker, or another professional in the service industry). Table \@ref(tab:pct-chg-groups-time-tibble) shows that changes to the percentage of the economy engaged in industry have been modest, with rich countries experiencing a slight deindustrialization and poor countries experiencing slight gains in industry employment However, the data also show that the decrease in the percentage of the economy engaged in agriculture is very positively correlated with income. Conversely, again, although that the absolute gains in the service industry may be modest in poor countries, in relative terms, the poorest countries were the ones that have experienced the most dramatic shifts to a service-based economy.

```{r gdpcap-time-plot, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 4, out.width = "100%", fig.align='center', fig.cap = "1991-2019 GDP per Capita by Income Group"}
GDP_yearly_average %>% 
  ggplot(aes(x = year, y = gdp, colour = `Income Group`)) +
  geom_point() +
  labs(xlab = "Year", ylab = "Mean GDP (PPP) per Capita", title = "GDP per Capita by Income Group") +
  theme_bw()
```

Figure \@ref(fig:gdpcap-time-plot) shows the limitations of relying on static, 2020 classifications of country by income group. We can see from the data that rich countries have grown enormously while poor countries have remained trapped in low levels of growth. This begs the question of whether or not these poor economies have really not experienced growth, or whether there have just been a number of poor countries that have "graduated" to the status of a richer country. And, looking at the above charts, if this phenomenon exists, would the poor countries that emphasize services over agriculture be the ones that graduate to rich countries, thus giving us the massive differences across income groups that we see? These questions and many more motivate [Part II: Economic Structure and Economic Growth], where I, quite ambitiously, attempt to assess how resource allocation affects economic growth over the short-to-medium term.

### Model 1: Recurrent Neural Network

Given that the relationship between all of these inputs and economic growth is highly nonlinear (if we could so easily rely on coefficients to determine `X` feature's contribution to economic growth, development economists would have solved the growth trap of least developed countries already!) I decided to build a neural network to model the relationship between resource distribution and economic outcomes. And, since the data is truly panel data - it is absolutely impossible to consider any response value without understanding the identity of the country that it pertains to as well as the `n` values that came before it temporally - I chose to create and train a Recurrent Neural Network. 

Considering the strength of the relationship between economic structure and GDP per capita found in [Exploratory Data Analysis (Data Exploration)], I decided to construct a very simple model that only used four input features to determine the output (real GDP per capita.) These four input features were: `employment in agriculture (% of total)`, `employment in industry (% of total)`, `employment in services (% of total)` and the year of the observation.

After wrangling the data into three dimensions, splicing it into sections of length 5 and then wrangling those into another three-dimensional array, I built a simple three layer recurrent neural network. It used ReLU activation functions at each step and, since the output was continuous, the output node was singular. The input shape was (5, 4) because it took 4 features (agriculture, industry, manufacturing, services and year) over 5 distinct years.

```{r model-pt1-summary, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 4, out.width = "100%", fig.align='center', fig.cap = "Summary of RNN Model from Part 1"}
summary(model_part_1)
```

```{r, cache = TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 4, out.width = "100%", fig.align='center', fig.cap = "Training History of RNN Model from Part 1"}
plot_model_history(model_part_1$history$history)
```
Using the handy imported `plot_model_history` function from class, we can see that the model learns quite a bit over the 100 epochs it's run. The predictions of the RNN model actually get quite good. Unfortunately, since the problem is not a classification one, there is no data to plot for the `accuracy` portion of this graph. 

Exactly how good are the predictions of the model?

```{r model1-abs-acc, cache=TRUE, eval=TRUE, echo=FALSE}
tribble(~abs_loss, as.numeric(evaluate(model_part_1, X_test_RNN_continuous_GDP, y_test_RNN_continuous_GDP, verbose = FALSE)[2])) %>%
  kable(format = "latex", row.names = NA,
                        booktabs = TRUE,
                        digits = 3,
                        col.names = c("Absolute Test Error"),
        caption = "Accuracy of RNN Model on Test Data") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")
```

The model, with very minimal tuning, Table \@ref(tab:model1-abs-acc) shows that this model can predict real GDP per capita within around $7500 USD. That's not perfect, but according to 2021 figures, that's substantially *less* than the difference between the US and Canada or the difference between the UK and Australia. It's, perhaps as expected, very directionally accurate with just the above four features.

Again, this model does not have very much tuning. I decided to save time to have the vast majority of tuning be for my models for [Part II: Economic Structure and Economic Growth], which is more involved and more relevant to what I wanted to study with this project. 

## Part II: Economic Structure and Economic Growth

This section is fundamentally different from the above [Part I: Economic Structure and Economic Development Level] in two ways. First, this section 

### Model 1: Linear Methods (Linear Models for Panel Data)

### Model 2: Recurrent Neural Network

Tune lag length!

# Conclusions

## Method Comparison

## Takeaways

## Limitations

## Follow-ups

The neural net performs much much better
- these relationships and dependencies I'm trying to model are anything but linear

